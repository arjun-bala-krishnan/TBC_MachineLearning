{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Version: 1.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.1.0\n",
      "Pandas Version: 1.0.1\n",
      "Tensorflow Version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import Final_Code_functions\n",
    "importlib.reload(Final_Code_functions)\n",
    "from Final_Code_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Directories Initialized Succesfully\n"
     ]
    }
   ],
   "source": [
    "# This snippet widens the jupyter notebook layout for coding\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "# We check and initialize all missing pre-process folder\n",
    "directory_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not place this in function or each reload of the function file will create a blank dataframe\n",
    "df_files = pd.DataFrame({'Sample': [],\n",
    "                'Image_Name': [],\n",
    "                'Image_ID':[],         \n",
    "                'Image_Type':[],\n",
    "                'Magnification': [],\n",
    "                'Image_Location': [],\n",
    "                'Image_Location_PostPocess': [],\n",
    "                'Image_Location_Interface': [],\n",
    "                'Image_Location_Threshold': []})\n",
    "\n",
    "Hg_porosity_df = pd.DataFrame({'Sample': str,\n",
    "                'Image_ID': str,\n",
    "                'Slice_ID':int,\n",
    "                'Radii': [],\n",
    "                'Ps': [],\n",
    "                'log_Radii': [],\n",
    "                'log_Radii_new': [],\n",
    "                'Ps_new': []})\n",
    "\n",
    "df_roughness = pd.DataFrame({'Sample': str,\n",
    "                       'Roughness_Type': [],\n",
    "                       'Lambda_c_(microns)': [],\n",
    "                       'Ra_(microns)': [],\n",
    "                       'Rq_(microns)': [],\n",
    "                       'Rz_(microns)': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\data\\PostProcess_Files_dataframe.json  to start Files and image data extraction..\n",
      "Delete  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\data\\PostProcess_Files_dataframe.json  to add a column of the image's reference image..\n",
      "Delete  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\data\\PostProcess_Files_dataframe.json  to apply CLAHE on all reference images and copy them to a separate folder called #Reference_Images..\n",
      "Delete  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\data\\PostProcess_Files_dataframe.json  to brightness equalise the images based on a reference image from the same subset..\n",
      "Delete  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\data\\PostProcess_Files_dataframe.json  to change filenames of all postprocess files with root dir as #Post_Process..\n",
      "Delete  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\data\\PostProcess_Files_dataframe.json  to apply simple Otsu threshold to the post processed files..\n",
      "Delete  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\data\\PostProcess_Files_dataframe.json  to change filenames of all threshold files with root dir as #Threshold..\n",
      "Delete  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\data\\PostProcess_Files_dataframe.json  to isolate the interfaces of the top portion..\n",
      "Delete  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\data\\PostProcess_Files_dataframe.json  to change filenames of all interface files with root dir as #Interfaces ..\n",
      "Delete  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\data\\PostProcess_Files_dataframe.json  get the Interface boundary from top and bottom portion of each microstructure..\n",
      "Delete  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\data\\PostProcess_Files_dataframe.json  to covert all non-existent interface files to NaN..\n",
      "Delete  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\data\\PostProcess_Files_dataframe.json  to save the completed files as a JSON file..\n"
     ]
    }
   ],
   "source": [
    "df_files = combined_df_files_operations(df_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>Image_Name</th>\n",
       "      <th>Image_ID</th>\n",
       "      <th>Image_Type</th>\n",
       "      <th>Magnification</th>\n",
       "      <th>Image_Location</th>\n",
       "      <th>Image_Location_PostPocess</th>\n",
       "      <th>Image_Location_Interface</th>\n",
       "      <th>Image_Location_Threshold</th>\n",
       "      <th>Reference_Image</th>\n",
       "      <th>Reference_CLAHE</th>\n",
       "      <th>Top_Boundary</th>\n",
       "      <th>Bottom_Boundary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M-19-071</td>\n",
       "      <td>M-19-071-TP0003.tif</td>\n",
       "      <td>JSC100</td>\n",
       "      <td>SEM_FreeStanding</td>\n",
       "      <td>500</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>329</td>\n",
       "      <td>1807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M-19-071</td>\n",
       "      <td>M-19-071-TP0004.tif</td>\n",
       "      <td>JSC101</td>\n",
       "      <td>SEM_FreeStanding</td>\n",
       "      <td>2000</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>None</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>2033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M-19-071</td>\n",
       "      <td>M-19-071-TP0005.tif</td>\n",
       "      <td>JSC102</td>\n",
       "      <td>SEM_FreeStanding</td>\n",
       "      <td>2000</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>None</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>2033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M-19-071</td>\n",
       "      <td>M-19-071-TP0006.tif</td>\n",
       "      <td>JSC103</td>\n",
       "      <td>SEM_FreeStanding</td>\n",
       "      <td>500</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>209</td>\n",
       "      <td>1751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M-19-071</td>\n",
       "      <td>M-19-071-TP0007.tif</td>\n",
       "      <td>JSC104</td>\n",
       "      <td>SEM_FreeStanding</td>\n",
       "      <td>2000</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>None</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>2033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sample           Image_Name Image_ID        Image_Type  Magnification  \\\n",
       "0  M-19-071  M-19-071-TP0003.tif   JSC100  SEM_FreeStanding            500   \n",
       "1  M-19-071  M-19-071-TP0004.tif   JSC101  SEM_FreeStanding           2000   \n",
       "2  M-19-071  M-19-071-TP0005.tif   JSC102  SEM_FreeStanding           2000   \n",
       "3  M-19-071  M-19-071-TP0006.tif   JSC103  SEM_FreeStanding            500   \n",
       "4  M-19-071  M-19-071-TP0007.tif   JSC104  SEM_FreeStanding           2000   \n",
       "\n",
       "                                      Image_Location  \\\n",
       "0  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "1  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "2  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "3  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "4  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "\n",
       "                           Image_Location_PostPocess  \\\n",
       "0  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "1  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "2  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "3  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "4  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "\n",
       "                            Image_Location_Interface  \\\n",
       "0  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "4                                               None   \n",
       "\n",
       "                            Image_Location_Threshold  \\\n",
       "0  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "1  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "2  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "3  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "4  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "\n",
       "                                     Reference_Image  \\\n",
       "0  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "1  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "2  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "3  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "4  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "\n",
       "                                     Reference_CLAHE  Top_Boundary  \\\n",
       "0  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...           329   \n",
       "1  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...             0   \n",
       "2  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...             0   \n",
       "3  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...           209   \n",
       "4  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...             0   \n",
       "\n",
       "   Bottom_Boundary  \n",
       "0             1807  \n",
       "1             2033  \n",
       "2             2033  \n",
       "3             1751  \n",
       "4             2033  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# view all the files updated so far\n",
    "display(df_files.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HotEncoding Powder_Name\n",
      "HotEncoding Type_of_Coating\n",
      "HotEncoding Powder_Size_Class\n",
      "HotEncoding Powder_Process\n",
      "HotEncoding Powder_Shape\n",
      "Delete  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\data\\Parameters_dataframe.json  to extract all powder and coating parameters and to save new..\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding for  catagorical parameters, combined to one input layer, percentile upper limit can be changed inside the function, default is 200 microns, after iteration we divide the powders into unit divisions with 200 as the divividing factor\n",
    "# Iteration to visualise best division for normalising powder diameter distribution. Existing powder classes are 0.1, 0.5 or 0.9    \n",
    "# iter_scale_powder_diameter(df_parameters, 0.9)\n",
    "df_parameters, df_parameters_encoded = combined_df_parameters_operations(max_powder_dia = 200, div = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_Name</th>\n",
       "      <th>Powder_Commercial_Name</th>\n",
       "      <th>Powder_Name</th>\n",
       "      <th>Type_of_Coating</th>\n",
       "      <th>Powder_Size_Class</th>\n",
       "      <th>Powder_Density_(g/cc)</th>\n",
       "      <th>Powder_Diameter_0.1_(microns)</th>\n",
       "      <th>Powder_Diameter_0.5_(microns)</th>\n",
       "      <th>Powder_Diameter_0.9_(microns)</th>\n",
       "      <th>Powder_Process</th>\n",
       "      <th>Powder_Shape</th>\n",
       "      <th>Thickness_(microns)</th>\n",
       "      <th>Weight_(g)</th>\n",
       "      <th>Spray_Distance_(mm)</th>\n",
       "      <th>Robot_Arm_Sweep_Velocity_(mm/sec)</th>\n",
       "      <th>Coating_Temperature_(Celsius)</th>\n",
       "      <th>Current_(A)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M-19-071</td>\n",
       "      <td>Metco204NS</td>\n",
       "      <td>YSZ412M</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Coarse</td>\n",
       "      <td>4.347</td>\n",
       "      <td>21.77133</td>\n",
       "      <td>53.92726</td>\n",
       "      <td>95.9551</td>\n",
       "      <td>Agglomerated+Densified</td>\n",
       "      <td>Hollow+Sphere</td>\n",
       "      <td>330</td>\n",
       "      <td>3.78</td>\n",
       "      <td>200</td>\n",
       "      <td>500</td>\n",
       "      <td>230</td>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M-19-074</td>\n",
       "      <td>Metco204NS</td>\n",
       "      <td>YSZ412M</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Coarse</td>\n",
       "      <td>4.347</td>\n",
       "      <td>21.77133</td>\n",
       "      <td>53.92726</td>\n",
       "      <td>95.9551</td>\n",
       "      <td>Agglomerated+Densified</td>\n",
       "      <td>Hollow+Sphere</td>\n",
       "      <td>350</td>\n",
       "      <td>3.99</td>\n",
       "      <td>200</td>\n",
       "      <td>500</td>\n",
       "      <td>230</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M-19-104</td>\n",
       "      <td>Metco204NS</td>\n",
       "      <td>YSZ412M</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Coarse</td>\n",
       "      <td>4.347</td>\n",
       "      <td>21.77133</td>\n",
       "      <td>53.92726</td>\n",
       "      <td>95.9551</td>\n",
       "      <td>Agglomerated+Densified</td>\n",
       "      <td>Hollow+Sphere</td>\n",
       "      <td>340</td>\n",
       "      <td>3.85</td>\n",
       "      <td>200</td>\n",
       "      <td>500</td>\n",
       "      <td>230</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M-19-267</td>\n",
       "      <td>Metco204NS</td>\n",
       "      <td>YSZ412M</td>\n",
       "      <td>Dense</td>\n",
       "      <td>Coarse</td>\n",
       "      <td>4.347</td>\n",
       "      <td>21.77133</td>\n",
       "      <td>53.92726</td>\n",
       "      <td>95.9551</td>\n",
       "      <td>Agglomerated+Densified</td>\n",
       "      <td>Hollow+Sphere</td>\n",
       "      <td>410</td>\n",
       "      <td>5.16</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>310</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M-19-269</td>\n",
       "      <td>Metco204NS</td>\n",
       "      <td>YSZ412M</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>Coarse</td>\n",
       "      <td>4.347</td>\n",
       "      <td>21.77133</td>\n",
       "      <td>53.92726</td>\n",
       "      <td>95.9551</td>\n",
       "      <td>Agglomerated+Densified</td>\n",
       "      <td>Hollow+Sphere</td>\n",
       "      <td>405</td>\n",
       "      <td>5.03</td>\n",
       "      <td>150</td>\n",
       "      <td>500</td>\n",
       "      <td>310</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sample_Name Powder_Commercial_Name Powder_Name Type_of_Coating  \\\n",
       "0    M-19-071             Metco204NS     YSZ412M        Standard   \n",
       "1    M-19-074             Metco204NS     YSZ412M        Standard   \n",
       "2    M-19-104             Metco204NS     YSZ412M        Standard   \n",
       "3    M-19-267             Metco204NS     YSZ412M           Dense   \n",
       "4    M-19-269             Metco204NS     YSZ412M    Intermediate   \n",
       "\n",
       "  Powder_Size_Class  Powder_Density_(g/cc)  Powder_Diameter_0.1_(microns)  \\\n",
       "0            Coarse                  4.347                       21.77133   \n",
       "1            Coarse                  4.347                       21.77133   \n",
       "2            Coarse                  4.347                       21.77133   \n",
       "3            Coarse                  4.347                       21.77133   \n",
       "4            Coarse                  4.347                       21.77133   \n",
       "\n",
       "   Powder_Diameter_0.5_(microns)  Powder_Diameter_0.9_(microns)  \\\n",
       "0                       53.92726                        95.9551   \n",
       "1                       53.92726                        95.9551   \n",
       "2                       53.92726                        95.9551   \n",
       "3                       53.92726                        95.9551   \n",
       "4                       53.92726                        95.9551   \n",
       "\n",
       "           Powder_Process   Powder_Shape  Thickness_(microns)  Weight_(g)  \\\n",
       "0  Agglomerated+Densified  Hollow+Sphere                  330        3.78   \n",
       "1  Agglomerated+Densified  Hollow+Sphere                  350        3.99   \n",
       "2  Agglomerated+Densified  Hollow+Sphere                  340        3.85   \n",
       "3  Agglomerated+Densified  Hollow+Sphere                  410        5.16   \n",
       "4  Agglomerated+Densified  Hollow+Sphere                  405        5.03   \n",
       "\n",
       "   Spray_Distance_(mm)  Robot_Arm_Sweep_Velocity_(mm/sec)  \\\n",
       "0                  200                                500   \n",
       "1                  200                                500   \n",
       "2                  200                                500   \n",
       "3                  100                                500   \n",
       "4                  150                                500   \n",
       "\n",
       "   Coating_Temperature_(Celsius)  Current_(A)  \n",
       "0                            230          419  \n",
       "1                            230          418  \n",
       "2                            230          418  \n",
       "3                            310          498  \n",
       "4                            310          458  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_Name</th>\n",
       "      <th>Powder_Density_(g/cc)</th>\n",
       "      <th>Powder_Diameter_0.1_(microns)</th>\n",
       "      <th>Powder_Diameter_0.5_(microns)</th>\n",
       "      <th>Powder_Diameter_0.9_(microns)</th>\n",
       "      <th>Thickness_(microns)</th>\n",
       "      <th>Weight_(g)</th>\n",
       "      <th>Spray_Distance_(mm)</th>\n",
       "      <th>Robot_Arm_Sweep_Velocity_(mm/sec)</th>\n",
       "      <th>Coating_Temperature_(Celsius)</th>\n",
       "      <th>...</th>\n",
       "      <th>Powder_Process__Agglomerated+Densified</th>\n",
       "      <th>Powder_Process__Agglomerated+Sintered</th>\n",
       "      <th>Powder_Process__Fused+Crushed</th>\n",
       "      <th>Powder_Shape__Hollow+Sphere</th>\n",
       "      <th>Powder_Shape__Solid+Angular</th>\n",
       "      <th>Powder_Name</th>\n",
       "      <th>Type_of_Coating</th>\n",
       "      <th>Powder_Size_Class</th>\n",
       "      <th>Powder_Process</th>\n",
       "      <th>Powder_Shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M-19-071</td>\n",
       "      <td>0.615862</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.783848</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.995025</td>\n",
       "      <td>0.998004</td>\n",
       "      <td>0.716511</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M-19-074</td>\n",
       "      <td>0.615862</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.831354</td>\n",
       "      <td>0.647727</td>\n",
       "      <td>0.995025</td>\n",
       "      <td>0.998004</td>\n",
       "      <td>0.716511</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M-19-104</td>\n",
       "      <td>0.615862</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.807601</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.995025</td>\n",
       "      <td>0.998004</td>\n",
       "      <td>0.716511</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M-19-267</td>\n",
       "      <td>0.615862</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.973872</td>\n",
       "      <td>0.837662</td>\n",
       "      <td>0.497512</td>\n",
       "      <td>0.998004</td>\n",
       "      <td>0.965732</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M-19-269</td>\n",
       "      <td>0.615862</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.961995</td>\n",
       "      <td>0.816558</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.998004</td>\n",
       "      <td>0.965732</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sample_Name Powder_Density_(g/cc) Powder_Diameter_0.1_(microns)  \\\n",
       "0    M-19-071              0.615862                          0.11   \n",
       "1    M-19-074              0.615862                          0.11   \n",
       "2    M-19-104              0.615862                          0.11   \n",
       "3    M-19-267              0.615862                          0.11   \n",
       "4    M-19-269              0.615862                          0.11   \n",
       "\n",
       "  Powder_Diameter_0.5_(microns) Powder_Diameter_0.9_(microns)  \\\n",
       "0                          0.27                          0.48   \n",
       "1                          0.27                          0.48   \n",
       "2                          0.27                          0.48   \n",
       "3                          0.27                          0.48   \n",
       "4                          0.27                          0.48   \n",
       "\n",
       "  Thickness_(microns) Weight_(g) Spray_Distance_(mm)  \\\n",
       "0            0.783848   0.613636            0.995025   \n",
       "1            0.831354   0.647727            0.995025   \n",
       "2            0.807601      0.625            0.995025   \n",
       "3            0.973872   0.837662            0.497512   \n",
       "4            0.961995   0.816558            0.746269   \n",
       "\n",
       "  Robot_Arm_Sweep_Velocity_(mm/sec) Coating_Temperature_(Celsius)  ...  \\\n",
       "0                          0.998004                      0.716511  ...   \n",
       "1                          0.998004                      0.716511  ...   \n",
       "2                          0.998004                      0.716511  ...   \n",
       "3                          0.998004                      0.965732  ...   \n",
       "4                          0.998004                      0.965732  ...   \n",
       "\n",
       "  Powder_Process__Agglomerated+Densified  \\\n",
       "0                                      1   \n",
       "1                                      1   \n",
       "2                                      1   \n",
       "3                                      1   \n",
       "4                                      1   \n",
       "\n",
       "   Powder_Process__Agglomerated+Sintered  Powder_Process__Fused+Crushed  \\\n",
       "0                                      0                              0   \n",
       "1                                      0                              0   \n",
       "2                                      0                              0   \n",
       "3                                      0                              0   \n",
       "4                                      0                              0   \n",
       "\n",
       "   Powder_Shape__Hollow+Sphere  Powder_Shape__Solid+Angular  Powder_Name  \\\n",
       "0                            1                            0            0   \n",
       "1                            1                            0            0   \n",
       "2                            1                            0            0   \n",
       "3                            1                            0            0   \n",
       "4                            1                            0            0   \n",
       "\n",
       "   Type_of_Coating  Powder_Size_Class  Powder_Process  Powder_Shape  \n",
       "0                2                  0               0             0  \n",
       "1                2                  0               0             0  \n",
       "2                2                  0               0             0  \n",
       "3                0                  0               0             0  \n",
       "4                1                  0               0             0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_parameters.head())\n",
    "display(df_parameters_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\data\\#Roughness\\roughsurface  is not empty\n",
      "C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\data\\#Roughness\\substrate  is not empty\n"
     ]
    }
   ],
   "source": [
    "# Extracting all roughness parameters\n",
    "df_roughness_first = df_roughness_data_extract()\n",
    "# extracting csv files and adding them to roughness dataframe only if the #Roughness folder is empty (takes time if started again)\n",
    "check_roughness_file()\n",
    "df_roughness_first['scan_csv_location']=df_roughness_first.apply(lambda x: update_csv_location(x['Sample'],x['Roughness_Type']),axis=1)\n",
    "# split the two sides of roughness into 2 dataframes. we need only rough_surface for Hg_Porosimetry\n",
    "df_roughness_interface_files_rough_surface, df_roughness_interface_files_substrate = associate_appropriate_scan_files(df_roughness_first, df_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_roughness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>Image_Location_Interface</th>\n",
       "      <th>Image_Location_PostPocess</th>\n",
       "      <th>Roughness_Type</th>\n",
       "      <th>Lambda_c_(microns)</th>\n",
       "      <th>Ra_(microns)</th>\n",
       "      <th>Rq_(microns)</th>\n",
       "      <th>Rz_(microns)</th>\n",
       "      <th>scan_csv_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M-19-267</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>Roughness_FreeStanding_Rough_Surface</td>\n",
       "      <td>2500</td>\n",
       "      <td>28833.2404929973</td>\n",
       "      <td>40118.6436314252</td>\n",
       "      <td>93073.7807206926</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M-19-267</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>Roughness_FreeStanding_Rough_Surface</td>\n",
       "      <td>2500</td>\n",
       "      <td>28833.2404929973</td>\n",
       "      <td>40118.6436314252</td>\n",
       "      <td>93073.7807206926</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M-19-267</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>Roughness_FreeStanding_Rough_Surface</td>\n",
       "      <td>2500</td>\n",
       "      <td>28833.2404929973</td>\n",
       "      <td>40118.6436314252</td>\n",
       "      <td>93073.7807206926</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M-19-267</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>Roughness_FreeStanding_Rough_Surface</td>\n",
       "      <td>2500</td>\n",
       "      <td>28833.2404929973</td>\n",
       "      <td>40118.6436314252</td>\n",
       "      <td>93073.7807206926</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M-19-267</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>Roughness_FreeStanding_Rough_Surface</td>\n",
       "      <td>2500</td>\n",
       "      <td>28833.2404929973</td>\n",
       "      <td>40118.6436314252</td>\n",
       "      <td>93073.7807206926</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>M-19-284</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>Roughness_FreeStanding_Rough_Surface</td>\n",
       "      <td>2500</td>\n",
       "      <td>5.78117388632864</td>\n",
       "      <td>7.28642281107629</td>\n",
       "      <td>40.6672678920796</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>M-19-284</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>Roughness_FreeStanding_Rough_Surface</td>\n",
       "      <td>2500</td>\n",
       "      <td>5.78117388632864</td>\n",
       "      <td>7.28642281107629</td>\n",
       "      <td>40.6672678920796</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>M-19-284</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>Roughness_FreeStanding_Rough_Surface</td>\n",
       "      <td>2500</td>\n",
       "      <td>5.78117388632864</td>\n",
       "      <td>7.28642281107629</td>\n",
       "      <td>40.6672678920796</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>M-19-284</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>Roughness_FreeStanding_Rough_Surface</td>\n",
       "      <td>2500</td>\n",
       "      <td>5.78117388632864</td>\n",
       "      <td>7.28642281107629</td>\n",
       "      <td>40.6672678920796</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>M-19-284</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>Roughness_FreeStanding_Rough_Surface</td>\n",
       "      <td>2500</td>\n",
       "      <td>5.78117388632864</td>\n",
       "      <td>7.28642281107629</td>\n",
       "      <td>40.6672678920796</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sample                           Image_Location_Interface  \\\n",
       "0   M-19-267  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "1   M-19-267  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "2   M-19-267  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "3   M-19-267  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "4   M-19-267  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "..       ...                                                ...   \n",
       "94  M-19-284  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "95  M-19-284  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "96  M-19-284  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "97  M-19-284  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "98  M-19-284  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "\n",
       "                            Image_Location_PostPocess  \\\n",
       "0   C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "1   C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "2   C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "3   C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "4   C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "..                                                ...   \n",
       "94  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "95  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "96  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "97  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "98  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "\n",
       "                          Roughness_Type Lambda_c_(microns)      Ra_(microns)  \\\n",
       "0   Roughness_FreeStanding_Rough_Surface               2500  28833.2404929973   \n",
       "1   Roughness_FreeStanding_Rough_Surface               2500  28833.2404929973   \n",
       "2   Roughness_FreeStanding_Rough_Surface               2500  28833.2404929973   \n",
       "3   Roughness_FreeStanding_Rough_Surface               2500  28833.2404929973   \n",
       "4   Roughness_FreeStanding_Rough_Surface               2500  28833.2404929973   \n",
       "..                                   ...                ...               ...   \n",
       "94  Roughness_FreeStanding_Rough_Surface               2500  5.78117388632864   \n",
       "95  Roughness_FreeStanding_Rough_Surface               2500  5.78117388632864   \n",
       "96  Roughness_FreeStanding_Rough_Surface               2500  5.78117388632864   \n",
       "97  Roughness_FreeStanding_Rough_Surface               2500  5.78117388632864   \n",
       "98  Roughness_FreeStanding_Rough_Surface               2500  5.78117388632864   \n",
       "\n",
       "        Rq_(microns)      Rz_(microns)  \\\n",
       "0   40118.6436314252  93073.7807206926   \n",
       "1   40118.6436314252  93073.7807206926   \n",
       "2   40118.6436314252  93073.7807206926   \n",
       "3   40118.6436314252  93073.7807206926   \n",
       "4   40118.6436314252  93073.7807206926   \n",
       "..               ...               ...   \n",
       "94  7.28642281107629  40.6672678920796   \n",
       "95  7.28642281107629  40.6672678920796   \n",
       "96  7.28642281107629  40.6672678920796   \n",
       "97  7.28642281107629  40.6672678920796   \n",
       "98  7.28642281107629  40.6672678920796   \n",
       "\n",
       "                                    scan_csv_location  \n",
       "0   C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...  \n",
       "1   C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...  \n",
       "2   C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...  \n",
       "3   C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...  \n",
       "4   C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...  \n",
       "..                                                ...  \n",
       "94  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...  \n",
       "95  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...  \n",
       "96  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...  \n",
       "97  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...  \n",
       "98  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...  \n",
       "\n",
       "[99 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_roughness_interface_files_rough_surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Version: 1.0.1\n",
      "Tensorflow Version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import Final_Code_functions\n",
    "importlib.reload(Final_Code_functions)\n",
    "from Final_Code_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_roughness (Scan Files) for slice generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use only the free surface to get slices currosponding to our porosimetry study, this will be joined to df_files and each csv file fed as input to get two forms of slices from scanned files\n",
    "# The 0 means the first row. change the row according to the position\n",
    "csv_file = df_roughness_interface_files_rough_surface['scan_csv_location'][0]\n",
    "output_roughness_image, slices_encoded_dict, slices_color_coded_dict = generate_roughness_slices(csv_file, sq_diam = 224, kernel = 5, \n",
    "                                                                                                     surface = \"Rough\", image = True, encoded = True, color_coded = False, seed = 5)\n",
    "# Apply True in the arguements to get the appropriate results. There is some minor error when trying to call both image and color coded file\n",
    "# view1_image(slices_color_coded_dict['1'])\n",
    "# view1_image(output_roughness_image)\n",
    "# display(slices_encoded_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df_roughness (interface images shadow property)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\data\\Roughness_dataframe.json  to start Files and image data extraction..\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>Image_Location_Interface</th>\n",
       "      <th>Image_Location_PostPocess</th>\n",
       "      <th>Roughness_Type</th>\n",
       "      <th>Lambda_c_(microns)</th>\n",
       "      <th>Ra_(microns)</th>\n",
       "      <th>Rq_(microns)</th>\n",
       "      <th>Rz_(microns)</th>\n",
       "      <th>scan_csv_location</th>\n",
       "      <th>shadow_average_x</th>\n",
       "      <th>...</th>\n",
       "      <th>shadow_average_y</th>\n",
       "      <th>shadow_density_y</th>\n",
       "      <th>undercuts_area_average_y</th>\n",
       "      <th>total_undercut_ratio</th>\n",
       "      <th>p10_steps</th>\n",
       "      <th>p50_steps</th>\n",
       "      <th>p90_steps</th>\n",
       "      <th>p10_undercut_ratio</th>\n",
       "      <th>p50_undercut_ratio</th>\n",
       "      <th>p90_undercut_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M-19-267</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>Roughness_FreeStanding_Rough_Surface</td>\n",
       "      <td>2500</td>\n",
       "      <td>28833.240493</td>\n",
       "      <td>40118.643631</td>\n",
       "      <td>93073.780721</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>12.379310</td>\n",
       "      <td>...</td>\n",
       "      <td>12.379310</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.293829</td>\n",
       "      <td>0.263129</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.5615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M-19-267</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>Roughness_FreeStanding_Rough_Surface</td>\n",
       "      <td>2500</td>\n",
       "      <td>28833.240493</td>\n",
       "      <td>40118.643631</td>\n",
       "      <td>93073.780721</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>10.261905</td>\n",
       "      <td>...</td>\n",
       "      <td>10.261905</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.257461</td>\n",
       "      <td>0.263129</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.5615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M-19-267</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>Roughness_FreeStanding_Rough_Surface</td>\n",
       "      <td>2500</td>\n",
       "      <td>28833.240493</td>\n",
       "      <td>40118.643631</td>\n",
       "      <td>93073.780721</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.145931</td>\n",
       "      <td>0.263129</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.5615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M-19-267</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>Roughness_FreeStanding_Rough_Surface</td>\n",
       "      <td>2500</td>\n",
       "      <td>28833.240493</td>\n",
       "      <td>40118.643631</td>\n",
       "      <td>93073.780721</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>17.357143</td>\n",
       "      <td>...</td>\n",
       "      <td>17.357143</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.079556</td>\n",
       "      <td>0.263129</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.5615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M-19-267</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>Roughness_FreeStanding_Rough_Surface</td>\n",
       "      <td>2500</td>\n",
       "      <td>28833.240493</td>\n",
       "      <td>40118.643631</td>\n",
       "      <td>93073.780721</td>\n",
       "      <td>C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...</td>\n",
       "      <td>12.218750</td>\n",
       "      <td>...</td>\n",
       "      <td>12.218750</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.242249</td>\n",
       "      <td>0.263129</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.5615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sample                           Image_Location_Interface  \\\n",
       "0  M-19-267  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "1  M-19-267  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "2  M-19-267  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "3  M-19-267  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "4  M-19-267  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "\n",
       "                           Image_Location_PostPocess  \\\n",
       "0  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "1  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "2  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "3  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "4  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...   \n",
       "\n",
       "                         Roughness_Type  Lambda_c_(microns)  Ra_(microns)  \\\n",
       "0  Roughness_FreeStanding_Rough_Surface                2500  28833.240493   \n",
       "1  Roughness_FreeStanding_Rough_Surface                2500  28833.240493   \n",
       "2  Roughness_FreeStanding_Rough_Surface                2500  28833.240493   \n",
       "3  Roughness_FreeStanding_Rough_Surface                2500  28833.240493   \n",
       "4  Roughness_FreeStanding_Rough_Surface                2500  28833.240493   \n",
       "\n",
       "   Rq_(microns)  Rz_(microns)  \\\n",
       "0  40118.643631  93073.780721   \n",
       "1  40118.643631  93073.780721   \n",
       "2  40118.643631  93073.780721   \n",
       "3  40118.643631  93073.780721   \n",
       "4  40118.643631  93073.780721   \n",
       "\n",
       "                                   scan_csv_location  shadow_average_x  ...  \\\n",
       "0  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...         12.379310  ...   \n",
       "1  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...         10.261905  ...   \n",
       "2  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...         10.400000  ...   \n",
       "3  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...         17.357143  ...   \n",
       "4  C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Fin...         12.218750  ...   \n",
       "\n",
       "   shadow_average_y  shadow_density_y  undercuts_area_average_y  \\\n",
       "0         12.379310          0.028320                  0.293829   \n",
       "1         10.261905          0.041016                  0.257461   \n",
       "2         10.400000          0.024414                  0.145931   \n",
       "3         17.357143          0.054688                  0.079556   \n",
       "4         12.218750          0.031250                  0.242249   \n",
       "\n",
       "   total_undercut_ratio  p10_steps  p50_steps  p90_steps  p10_undercut_ratio  \\\n",
       "0              0.263129          4          8         32            0.008361   \n",
       "1              0.263129          4          8         32            0.008361   \n",
       "2              0.263129          4          8         32            0.008361   \n",
       "3              0.263129          4          8         32            0.008361   \n",
       "4              0.263129          4          8         32            0.008361   \n",
       "\n",
       "   p50_undercut_ratio  p90_undercut_ratio  \n",
       "0                0.24              0.5615  \n",
       "1                0.24              0.5615  \n",
       "2                0.24              0.5615  \n",
       "3                0.24              0.5615  \n",
       "4                0.24              0.5615  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df_master_interface_sample will have the master dataframes used to calculate 'step' or distribution properties common to each sample\n",
    "# df_master_interface_sample will have the local parameters such as average and density which is different for each of the interface profiles\n",
    "# both of them will be combined into the original dataframe passed into this function: df_roughness_interface_files_rough_surface. The JSON file will be saved\n",
    "\n",
    "if (os.path.isfile(df_files_roughness_file)):\n",
    "    df_roughness_interface_files_rough_surface = pd.read_json(df_files_roughness_file, orient = 'columns') \n",
    "    df_roughness_interface_files_rough_surface.sort_index(inplace=True)\n",
    "    print(\"Delete \", df_files_roughness_file, \" to start Files and image data extraction..\")\n",
    "else:\n",
    "    df_master_interface_sample, dict_interface_parameteters = make_interface_shadow_parameters(df_roughness_interface_files_rough_surface)\n",
    "    df_roughness_interface_files_rough_surface2 = pd.merge(df_roughness_interface_files_rough_surface, df_interface_parameteters, on='Image_Location_Interface')\n",
    "    step_distribution_parameters = find_step_distribution_parameters(df_roughness_interface_files_rough_surface, df_master_interface_sample)\n",
    "    df_roughness_interface_files_rough_surface = pd.merge(df_roughness_interface_files_rough_surface2, step_distribution_parameters, on='Sample')\n",
    "    df_roughness_interface_files_rough_surface.to_json(df_files_roughness_file, orient='columns')\n",
    "    \n",
    "display(df_roughness_interface_files_rough_surface.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hg_df with 5, 10 and 15 classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataframe has Ps_new which is the result of an interpolation to log_Radii_new prepared in a custom range (global minimum and global maximum) using linspace\n",
    "# We extract mercury porosimetry results using dictionary as output \n",
    "# datas_ = list of [sample_name, array of values Radius, array of values of Porosity%]\n",
    "# Hg_df_dict = key: sample_name, value: pandas dataframe containing Radius and Porosity% \n",
    "# use df_files_new to avoid bad/duplicate referenceing inside the functions file\n",
    "\n",
    "df_files_new = copy.deepcopy(df_files)\n",
    "datas_, Hg_df_dict = df_mercury_data_extract()\n",
    "df_Hg_porosity_5 = df_mercury_common_cordinates(datas_, Hg_df_dict, df_files_new, num = 5, kind = \"linear\")\n",
    "df_Hg_porosity_5 = df_Hg_porosity_5.drop_duplicates(subset=['Sample'])\n",
    "df_Hg_porosity_5 = df_Hg_porosity_5.reset_index()\n",
    "df_Hg_porosity_5 = df_Hg_porosity_5.drop(columns = 'index')\n",
    "\n",
    "df_files_new = copy.deepcopy(df_files)\n",
    "datas_, Hg_df_dict = df_mercury_data_extract()\n",
    "df_Hg_porosity_10 = df_mercury_common_cordinates(datas_, Hg_df_dict, df_files_new, num = 10, kind = \"linear\")\n",
    "df_Hg_porosity_10 = df_Hg_porosity_10.drop_duplicates(subset=['Sample'])\n",
    "df_Hg_porosity_10 = df_Hg_porosity_10.reset_index()\n",
    "df_Hg_porosity_10 = df_Hg_porosity_10.drop(columns = 'index')\n",
    "\n",
    "df_files_new = copy.deepcopy(df_files)\n",
    "datas_, Hg_df_dict = df_mercury_data_extract()\n",
    "df_Hg_porosity_15 = df_mercury_common_cordinates(datas_, Hg_df_dict, df_files_new, num = 15, kind = \"linear\")\n",
    "df_Hg_porosity_15 = df_Hg_porosity_15.drop_duplicates(subset=['Sample'])\n",
    "df_Hg_porosity_15 = df_Hg_porosity_15.reset_index()\n",
    "df_Hg_porosity_15 = df_Hg_porosity_15.drop(columns = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Reshape, BatchNormalization, MaxPool2D, Convolution2D, InputLayer, Flatten, Concatenate, Lambda, Activation\n",
    "from keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import winsound\n",
    "\n",
    "\n",
    "# function to make sure that the output layer is organised as a cumulative sum\n",
    "def Cumulative_Sum(inp):\n",
    "    return K.reverse(K.cumsum(inp,axis=1),axes=1)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roughness Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roughness interface image slice + scan slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roughness = df_roughness_interface_files_rough_surface.merge(df_Hg_porosity_10, left_on='Sample', right_on='Sample')\n",
    "df_roughness = df_roughness.drop(columns = \"Radii\")\n",
    "df_roughness = df_roughness.drop(columns = \"Ps\")\n",
    "df_roughness = df_roughness.drop(columns = \"log_Radii\")\n",
    "df_roughness = df_roughness.drop(columns = \"Roughness_Type\")\n",
    "# df_roughness = df_roughness.drop(columns = \"Lambda_c_(microns)\")\n",
    "# df_roughness = df_roughness.drop(columns = \"Ra_(microns)\")\n",
    "# df_roughness = df_roughness.drop(columns = \"Rq_(microns)\")\n",
    "# df_roughness = df_roughness.drop(columns = \"Rz_(microns)\")\n",
    "# df_roughness = df_roughness.drop(columns = \"shadow_density_y\")\n",
    "# df_roughness = df_roughness.drop(columns = \"shadow_average_y\")\n",
    "# df_roughness = df_roughness.drop(columns = \"undercuts_area_average_y\")\n",
    "Hg_roughness = (np.array(df_roughness[\"Ps_new\"].tolist())) / 100.\n",
    "df_roughness_train, df_500_roughness_holdout, Y_Hg_train, Y_Hg_holdout = train_test_split(df_roughness, Hg_roughness, shuffle = True, random_state = 12, test_size = 0.1)\n",
    "\n",
    "# len(df_roughness.p90_undercut_ratio.unique())\n",
    "# display(np.where(df_roughness['shadow_density_x'] == df_roughness['shadow_density_y'], 'no change', 'changed'))\n",
    "\n",
    "scan_loc_train = np.array(df_roughness_train[\"scan_csv_location\"].tolist(), dtype = 'U')\n",
    "scan_loc_holdout = np.array(df_roughness_train[\"scan_csv_location\"].tolist(), dtype = 'U')\n",
    "interface_loc_train = np.array(df_roughness_train[\"Image_Location_Interface\"].tolist(), dtype = 'U')\n",
    "interface_loc_holdout = np.array(df_500_roughness_holdout[\"Image_Location_Interface\"].tolist(), dtype = 'U')\n",
    "interface_train = []\n",
    "interface_holdout = []\n",
    "\n",
    "for loc in interface_loc_train:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    interface_train.append(img)\n",
    "    \n",
    "for loc in interface_loc_holdout:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    interface_holdout.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_roughness_only_model(nfilter = 16, filter_size = 3):   \n",
    "    model1 = Sequential()\n",
    "    model1.add(InputLayer(input_shape=(224,224), name= \"interface_slice_input\"))\n",
    "    model1.add(Reshape((224,224,1)))\n",
    "    model1.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\", activation=\"relu\"))\n",
    "    model1.add(MaxPool2D())\n",
    "    model1.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\", activation=\"relu\"))\n",
    "    model1.add(MaxPool2D())\n",
    "    model1.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\", activation=\"relu\"))\n",
    "    model1.add(MaxPool2D())\n",
    "    model1.add(Flatten())\n",
    "    \n",
    "    model2 = Sequential()\n",
    "    model2.add(InputLayer(input_shape=(224, 224), name=\"interface_scan_slice_input\"))\n",
    "    model2.add(Reshape((224,224,1)))\n",
    "    model2.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\", activation=\"relu\"))\n",
    "    model2.add(MaxPool2D())\n",
    "    model2.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\", activation=\"relu\"))\n",
    "    model2.add(MaxPool2D())\n",
    "    model2.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\", activation=\"relu\"))\n",
    "    model2.add(MaxPool2D())\n",
    "    model2.add(Flatten())\n",
    "    \n",
    "    im_concat_layer = Concatenate()([model1.output,model2.output])\n",
    "    dense_im = Dense(200, activation=\"relu\")(im_concat_layer)\n",
    "    dense_2 = Dense(10, activation=\"relu\",name= \"Hg_porosity_diff\")(dense_im)\n",
    "    lambda_layer = Lambda(Cumulative_Sum,name= \"Hg_porosity\")(dense_2)\n",
    "    \n",
    "    big_model = Model([model1.input, model2.input],[lambda_layer])\n",
    "    big_model.compile(\"adam\",loss=\"mean_squared_error\", metrics = [\"mae\"])\n",
    "    return big_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_only_roughness_patience5 = build_roughness_only_model(nfilter = 16, filter_size=3)\n",
    "# image_only_roughness_patience15 = build_roughness_only_model(nfilter = 16, filter_size=3)\n",
    "# image_only_roughness_patience15_weighted = build_roughness_only_model(nfilter = 16, filter_size=3)\n",
    "# image_only_roughness_patience20 = build_roughness_only_model(nfilter = 16, filter_size=3)\n",
    "# image_only_roughness_patience25_weighted = build_roughness_only_model(nfilter = 16, filter_size=3)\n",
    "\n",
    "# plot_model(image_only_roughness, \"build_roughness_only_model.png\", show_shapes=False, show_layer_names=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator function for slicing on train, validation and test\n",
    "def RoughnessImageGenerator(X_Train_interface, X_Train_scan, Y_Train_hg, df_train, img_diam = 224, batch_size = 100, num_batches = 100, slice_per_image = 2, seed = None):\n",
    "    \n",
    "    def slice_image_interface(img, diam, seed = None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)  \n",
    "\n",
    "        pass_bit = False\n",
    "        while(pass_bit is not True):\n",
    "            rand_row = np.random.randint(0, img.shape[0] - diam - 1)\n",
    "            rand_col = np.random.randint(0, img.shape[1] - diam - 1)\n",
    "\n",
    "            cropped = img[rand_row:rand_row + diam, rand_col:rand_col + diam]\n",
    "            pass_count = 0\n",
    "            for i in range(cropped.shape[0]):\n",
    "                avg = np.mean(cropped[i])\n",
    "                if avg > 0:\n",
    "                    pass_count = pass_count + 1\n",
    "\n",
    "                if pass_count > 50:\n",
    "                    pass_bit = True  \n",
    "                    break\n",
    "                else:\n",
    "                    pass_bit = False\n",
    "                    rand_row = np.random.randint(0, img.shape[0] - diam - 1)\n",
    "                    rand_col = np.random.randint(0, img.shape[1] - diam - 1)\n",
    "        return cropped\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)  \n",
    "        \n",
    "    tot_num_images = len(X_Train_interface) \n",
    "    batch_iteration_list = [np.random.choice(range(tot_num_images), batch_size, replace=True)\n",
    "                            for _ in range(num_batches)]\n",
    "      \n",
    "    while True: # Loop over epochs         \n",
    "        for batch_index in range(num_batches):       # Loop over batches within one epoch\n",
    "            slice_batch_trial1 = []\n",
    "            hg_porosity_batch = []\n",
    "            \n",
    "            img_slice_norm_list = []\n",
    "            img_scan_slice_norm_list = []\n",
    "            \n",
    "            rand_list = batch_iteration_list[batch_index]\n",
    "            \n",
    "            i = 0\n",
    "            while i < batch_size:                    # Loop over one batch\n",
    "                rand_index = rand_list[i]\n",
    "                for _ in range(slice_per_image):     # Loop over one image\n",
    "                    \n",
    "                    # generates a 224x224 slice into img_slice\n",
    "                    img_slice = slice_image_interface(X_Train_interface[rand_index], img_diam)\n",
    "                    # generates the second 224x224 slice into output_roughness_image\n",
    "                    output_roughness_image, slices_encoded_dict, slices_color_coded_dict = generate_roughness_slices(X_Train_scan[rand_index], sq_diam = 224, kernel = 5, \n",
    "                                                                                                     surface = \"Rough\", image = True, encoded = False, color_coded = False, seed = None)\n",
    "                    \n",
    "                    img_slice_norm = np.array(img_slice/255.)\n",
    "                    img_scan_slice_norm = np.array(output_roughness_image/255.)\n",
    "                    hg_porosity_batch.append(Y_Train_hg[rand_index])\n",
    "            \n",
    "                    img_slice_norm_list.append(img_slice_norm)\n",
    "                    img_scan_slice_norm_list.append(img_scan_slice_norm)\n",
    "                    \n",
    "                i = i + 1 \n",
    "            \n",
    "            final_list = [np.array(img_slice_norm_list),np.array(img_scan_slice_norm_list)]\n",
    "            hg_porosity_batch = np.array(hg_porosity_batch)\n",
    "            norm_weights = np.abs(hg_porosity_batch[:,0] - 0.07)*100+10\n",
    "            norm_weights = norm_weights/np.sum(norm_weights)\n",
    "            norm_weights = norm_weights*len(norm_weights)\n",
    "#             yield final_list, np.array(hg_porosity_batch)\n",
    "            yield final_list, hg_porosity_batch, norm_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model = os.path.join(path_Models, 'image_only_roughness_patience15_weighted.h5')\n",
    "if not os.path.isfile(file_model):\n",
    "    open(file_model, 'a').close()\n",
    "    \n",
    "checkpoint = ModelCheckpoint(file_model, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "callbacks_control = keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint, callbacks_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_list.shape =  (2, 10, 224, 224) test_poros.shape =  (10, 10)\n"
     ]
    }
   ],
   "source": [
    "# Test data generated\n",
    "num = 10\n",
    "test_generator = RoughnessImageGenerator(interface_holdout, scan_loc_holdout, Y_Hg_holdout, df_roughness, \n",
    "                         img_diam = 224, batch_size = 1, num_batches = num, \n",
    "                         slice_per_image = 1, seed = None)\n",
    "\n",
    "test_data = [result for i, result in zip(range(num), test_generator)]\n",
    "\n",
    "test_img_slice_norm_list = []\n",
    "test_img_scan_slice_norm_list = []\n",
    "test_poros = []\n",
    "\n",
    "for img, poros, _ in (test_data):\n",
    "    test_img_slice_norm_list.append(img[0])\n",
    "    test_img_scan_slice_norm_list.append(img[1])\n",
    "    test_poros.append(poros)\n",
    "    \n",
    "test_img_slice_norm_list = np.array(test_img_slice_norm_list)\n",
    "test_img_scan_slice_norm_list = np.array(test_img_scan_slice_norm_list)\n",
    "\n",
    "test_img_slice_norm_list = test_img_slice_norm_list.reshape(test_img_slice_norm_list.shape[0], test_img_slice_norm_list.shape[2],test_img_slice_norm_list.shape[3])\n",
    "test_img_scan_slice_norm_list = test_img_scan_slice_norm_list.reshape(test_img_scan_slice_norm_list.shape[0], test_img_scan_slice_norm_list.shape[2],test_img_scan_slice_norm_list.shape[3])\n",
    "final_test_list = [test_img_slice_norm_list, test_img_scan_slice_norm_list]\n",
    "\n",
    "test_poros = np.array(test_poros)\n",
    "test_poros = test_poros.reshape(len(test_poros),test_poros.shape[-1])\n",
    "\n",
    "print(\"final_list.shape = \",np.shape(final_test_list), \"test_poros.shape = \",np.shape(test_poros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the number of epochs and batch size\n",
    "BS = 25\n",
    "EPOCHS = 50\n",
    "VS = 0.15\n",
    "\n",
    "# construct the training image generator for data augmentation\n",
    "train_generator = RoughnessImageGenerator(interface_train, scan_loc_train, Y_Hg_train, df_roughness,\n",
    "                         img_diam = 224, batch_size = 10, num_batches = BS, \n",
    "                         slice_per_image = 1, seed = None)\n",
    "\n",
    "validation_generator = RoughnessImageGenerator(interface_train, scan_loc_train, Y_Hg_train, df_roughness, \n",
    "                         img_diam = 224, batch_size = 5, num_batches = 5, \n",
    "                         slice_per_image = 1, seed = None)\n",
    "\n",
    "\n",
    "history_image_only_roughness_patience15_weighted = image_only_roughness_patience15_weighted.fit_generator(train_generator, validation_data = validation_generator, validation_steps = 5,\n",
    "                        steps_per_epoch = BS, epochs=EPOCHS, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(history_image_only_roughness_patience15_weighted.history[\"loss\"],label=\"Train\", linewidth=2)\n",
    "plt.plot(history_image_only_roughness_patience15_weighted.history[\"val_loss\"],label=\"Validation\", linewidth=2)\n",
    "plt.legend(fontsize=18)\n",
    "plt.ylabel(\"MAE Loss\", fontsize=18)\n",
    "plt.xlabel(\"Epochs\", fontsize=18)\n",
    "# plt.savefig(\"history_roughness2.png\")\n",
    "\n",
    "print(signaltonoise(history_image_only_roughness_patience15_weighted.history[\"mae\"][-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_25_weighted_additional_50_final = load_model(r'C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\Data\\#Models\\Finsished\\image_only_roughness_patience25_weighted_additional50.h5')\n",
    "# y_pred_5 = image_only_roughness_patience5.predict(final_test_list)\n",
    "# y_pred_15 = image_only_roughness_patience15.predict(final_test_list)\n",
    "# y_pred_20 = image_only_roughness_patience20.predict(final_test_list)\n",
    "# y_pred_25_weighted = image_only_roughness_patience25_weighted.predict(final_test_list)\n",
    "y_pred_max_weighted = image_only_roughness_patience15_weighted.predict(final_test_list)\n",
    "y_pred_25_weighted_new = y_pred_25_weighted_additional_50_final.predict(final_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_25_weighted_additional_50_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers = 0\n",
    "# plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "# for y_pred_i, y_true_i in zip(y_pred_20, Y_Hg_holdout):\n",
    "#     numbers = numbers +1\n",
    "#     if numbers <10:\n",
    "#         plt.plot(y_pred_i, label=\"Predicted Curve \" + str(numbers))\n",
    "#         plt.plot(y_true_i,label=\"Original Curve \"+ str(numbers))\n",
    "#         plt.ylabel(\"Porosity ratio\", fontsize=18)\n",
    "#         plt.xlabel(\"log(Radii)\", fontsize=18)\n",
    "#         plt.legend(fontsize=18)\n",
    "#         plt.show()\n",
    "#         plt.pause(0)\n",
    "# #         plt.savefig(\"y_pred_25_weighted_new\"+str(numbers)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_25_weighted_additional_50_final = load_model(r'C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\Data\\#Models\\Finsished\\image_only_roughness_patience25_weighted_additional50.h5')\n",
    "y_pred_5 = image_only_roughness_patience5.predict(final_test_list)\n",
    "y_pred_15 = image_only_roughness_patience15.predict(final_test_list)\n",
    "y_pred_20 = image_only_roughness_patience20.predict(final_test_list)\n",
    "y_pred_25_weighted = image_only_roughness_patience25_weighted.predict(final_test_list)\n",
    "y_pred_max_weighted = image_only_roughness_patience15_weighted.predict(final_test_list)\n",
    "y_pred_25_weighted_new = y_pred_25_weighted_additional_50_final.predict(final_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(history_image_only_roughness_patience25_weighted.history[\"val_loss\"]))\n",
    "print(np.min(history_image_only_roughness_patience25_weighted.history[\"mae\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_original_error(pred, orig, div = 10):\n",
    "    pred = pred*100\n",
    "    orig = orig*100\n",
    "    error = np.subtract(pred, orig)\n",
    "    error_abs_diff = np.abs(error)\n",
    "    mean = np.mean(error_abs_diff)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_percent = 0\n",
    "for y_pred_i, y_true_i in zip(y_pred_25_weighted_new, Y_Hg_holdout):\n",
    "    MAE_Total = MAE_percent + prediction_original_error(y_pred_i, y_true_i)\n",
    "    \n",
    "print(MAE_Total)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roughness with scalar shadow property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roughness = df_roughness_interface_files_rough_surface.merge(df_Hg_porosity_10, left_on='Sample', right_on='Sample')\n",
    "df_roughness = df_roughness.drop(columns = \"Radii\")\n",
    "df_roughness = df_roughness.drop(columns = \"Ps\")\n",
    "df_roughness = df_roughness.drop(columns = \"log_Radii\")\n",
    "df_roughness = df_roughness.drop(columns = \"Roughness_Type\")\n",
    "df_roughness = df_roughness.drop(columns = \"Lambda_c_(microns)\")\n",
    "df_roughness = df_roughness.drop(columns = \"Ra_(microns)\")\n",
    "df_roughness = df_roughness.drop(columns = \"Rq_(microns)\")\n",
    "df_roughness = df_roughness.drop(columns = \"Rz_(microns)\")\n",
    "df_roughness = df_roughness.drop(columns = \"shadow_density_y\")\n",
    "df_roughness = df_roughness.drop(columns = \"shadow_average_y\")\n",
    "df_roughness = df_roughness.drop(columns = \"undercuts_area_average_y\")\n",
    "\n",
    "df_roughness = df_roughness.drop(columns = \"p10_steps\")\n",
    "df_roughness = df_roughness.drop(columns = \"p50_steps\")\n",
    "df_roughness = df_roughness.drop(columns = \"p90_steps\")\n",
    "df_roughness = df_roughness.drop(columns = \"p10_undercut_ratio\")\n",
    "df_roughness = df_roughness.drop(columns = \"p50_undercut_ratio\")\n",
    "df_roughness = df_roughness.drop(columns = \"p90_undercut_ratio\")\n",
    "\n",
    "df_roughness[\"shadow_average_scaled\"] = (df_roughness[\"shadow_average_x\"] - df_roughness['shadow_average_x'].min())/(df_roughness['shadow_average_x'].max() - df_roughness['shadow_average_x'].min())\n",
    "df_roughness[\"shadow_density_scaled\"] = (df_roughness[\"shadow_density_x\"] - df_roughness['shadow_density_x'].min())/(df_roughness['shadow_density_x'].max() - df_roughness['shadow_density_x'].min())   \n",
    "df_roughness[\"undercuts_area_average_scaled\"] = (df_roughness[\"undercuts_area_average_x\"] - df_roughness['undercuts_area_average_x'].min())/(df_roughness['undercuts_area_average_x'].max() - df_roughness['undercuts_area_average_x'].min())   \n",
    "df_roughness = df_roughness.drop(columns = \"shadow_average_x\")\n",
    "df_roughness = df_roughness.drop(columns = \"shadow_density_x\")\n",
    "df_roughness = df_roughness.drop(columns = \"undercuts_area_average_x\")\n",
    "df_roughness = df_roughness.drop(columns = \"total_undercut_ratio\")\n",
    "\n",
    "\n",
    "Hg_roughness = (np.array(df_roughness[\"Ps_new\"].tolist())) / 100.\n",
    "df_roughness_train, df_500_roughness_holdout, Y_Hg_train, Y_Hg_holdout = train_test_split(df_roughness, Hg_roughness, shuffle = True, random_state = 12, test_size = 0.1)\n",
    "\n",
    "# len(df_roughness.p90_undercut_ratio.unique())\n",
    "# display(np.where(df_roughness['shadow_density_x'] == df_roughness['shadow_density_y'], 'no change', 'changed'))\n",
    "\n",
    "scan_loc_train = np.array(df_roughness_train[\"scan_csv_location\"].tolist(), dtype = 'U')\n",
    "scan_loc_holdout = np.array(df_roughness_train[\"scan_csv_location\"].tolist(), dtype = 'U')\n",
    "interface_loc_train = np.array(df_roughness_train[\"Image_Location_Interface\"].tolist(), dtype = 'U')\n",
    "interface_loc_holdout = np.array(df_500_roughness_holdout[\"Image_Location_Interface\"].tolist(), dtype = 'U')\n",
    "interface_train = []\n",
    "interface_holdout = []\n",
    "\n",
    "for loc in interface_loc_train:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    interface_train.append(img)\n",
    "    \n",
    "for loc in interface_loc_holdout:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    interface_holdout.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roughness.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roughness.scan_csv_location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Train_Hg_porosity = np.array(df_roughness[\"Ps_new\"].tolist())\n",
    "Y_Train_Hg_porosity = Y_Train_Hg_porosity / 100.\n",
    "\n",
    "df_train, df_holdout, Y_Train_Hg_porosity_train, Y_Train_Hg_porosity_holdout = train_test_split(df_roughness, Y_Train_Hg_porosity, shuffle = True, random_state = None, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Y_Train_Hg_porosity.shape[0] == Y_Train_Hg_porosity_train.shape[0] + Y_Train_Hg_porosity_holdout.shape[0]\n",
    "assert df_roughness.shape[0] ==df_train.shape[0] + df_holdout.shape[0]\n",
    "assert Y_Train_Hg_porosity_train.shape[0] == df_train.shape[0]\n",
    "assert df_holdout.shape[0] == Y_Train_Hg_porosity_holdout.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_shadow_train =df_train[[\"shadow_average_scaled\",\"shadow_density_scaled\",\"undercuts_area_average_scaled\"]].values\n",
    "X_Train_shadow_holdout =df_holdout[[\"shadow_average_scaled\",\"shadow_density_scaled\",\"undercuts_area_average_scaled\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Train_Hg_porosity_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_shadow_ony_model(): \n",
    "    model1 = Sequential()\n",
    "    model1.add(InputLayer(input_shape=(3,), name=\"shadow\"))\n",
    "    model1.add(Dense(100, activation=\"relu\"))\n",
    "\n",
    "    dense_1 = Dense(100, activation=\"relu\")(model1.output)\n",
    "    dense_2 = Dense(10, activation=\"relu\",name= \"Hg_porosity_diff\")(dense_1)\n",
    "    lambda_layer = Lambda(Cumulative_Sum,name= \"Hg_porosity\")(dense_2)\n",
    "       \n",
    "    big_model = Model(model1.input,[lambda_layer])    \n",
    "    big_model.compile(\"adam\",loss=\"mean_squared_error\", metrics = [\"mae\"])\n",
    "    return big_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_only_model = build_shadow_ony_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model =  r'C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\Data\\#Models\\From_Learning_Complete(BackupSlicePorosity)\\shadow_only_model.h5'\n",
    "if not os.path.isfile(file_model):\n",
    "    open(file_model, 'a').close()\n",
    "    \n",
    "checkpoint = ModelCheckpoint(file_model, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "callbacks_control = keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=1000, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint, callbacks_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_shadow= shadow_only_model.fit(x=X_Train_shadow_train, y = Y_Train_Hg_porosity_train, \n",
    "                                           batch_size=62, validation_split=0.15, epochs=10000, verbose=2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = shadow_only_model.predict(X_Train_shadow_holdout) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(history_shadow.history[\"mae\"],label=\"Train\", linewidth=2)\n",
    "plt.plot(history_shadow.history[\"val_mae\"],label=\"Validation\", linewidth=2)\n",
    "plt.legend(fontsize=18)\n",
    "plt.ylabel(\"MAE Loss\", fontsize=18)\n",
    "plt.xlabel(\"Epochs\", fontsize=18)\n",
    "plt.savefig(r\"C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\Files_Save_Backup\\Models_documentation\\ShadowOnly\\ShadowOnly.png\")\n",
    "\n",
    "# print(signaltonoise(history_image_only_roughness_patience15_weighted.history[\"mae\"][-10:]))\n",
    "\n",
    "\n",
    "# plt.plot(history_shadow.history[\"mae\"],label=\"train\")\n",
    "# plt.plot(history_shadow.history[\"val_mae\"],label=\"validaition\")\n",
    "# plt.legend()\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for y_pred_i, y_true_i in zip(y_pred, Y_Train_Hg_porosity_holdout):\n",
    "    plt.plot(y_pred_i, label=\"pred\")\n",
    "    plt.plot(y_true_i,label=\"True\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_roughness_shadow_model(nfilter = 16, filter_size = 3):   \n",
    "    model1 = Sequential()\n",
    "    model1.add(InputLayer(input_shape=(224,224), name= \"interface_scan_slice_input\"))\n",
    "    model1.add(Reshape((224,224,1)))\n",
    "    model1.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\", activation=\"relu\"))\n",
    "    model1.add(MaxPool2D())\n",
    "    model1.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\", activation=\"relu\"))\n",
    "    model1.add(MaxPool2D())\n",
    "    model1.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\", activation=\"relu\"))\n",
    "    model1.add(MaxPool2D())\n",
    "    model1.add(Flatten())\n",
    "    \n",
    "    model2 = Sequential()\n",
    "    model2.add(InputLayer(input_shape=(3,), name= \"shadow_input\"))\n",
    "    model2.add(Dense(10, activation=\"relu\"))\n",
    "    \n",
    "    dense_shadow = Dense(5, activation=\"relu\")(model2.output)\n",
    "    dense_im = Dense(5, activation=\"relu\")(model1.output)\n",
    "    concat_layer = Concatenate(name= \"Hg_porosity_diff\")([dense_shadow, dense_im])\n",
    "    lambda_layer = Lambda(Cumulative_Sum, name= \"Hg_porosity\")(concat_layer)\n",
    "    \n",
    "    big_model = Model([model1.input, model2.input],[lambda_layer])    \n",
    "    big_model.compile(\"adam\",loss=\"mean_squared_error\", metrics = [\"mae\"])\n",
    "    return big_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roughness_shadow_model = build_roughness_shadow_model(nfilter = 16, filter_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model = os.path.join(path_Models, 'roughness_shadow_model.h5')\n",
    "if not os.path.isfile(file_model):\n",
    "    open(file_model, 'a').close()\n",
    "    \n",
    "checkpoint = ModelCheckpoint(file_model, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "callbacks_control = keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint, callbacks_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_roughness_shadow_model = roughness_shadow_model.fit(x=[X_Train_bulk_img_train,X_Train_porosity_train], \n",
    "                        y = Y_Train_Hg_porosity_train, batch_size=25,validation_split=0.15, \n",
    "                        epochs=500,verbose=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Only Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Designed (must be changed for each of the following models)\n",
    "def build_image_only_model(nfilter = 32, filter_size = 3):   \n",
    "    model1 = Sequential()\n",
    "    model1.add(InputLayer(input_shape=(224,224), name= \"bulk_img_input\"))\n",
    "    model1.add(Reshape((224,224,1)))\n",
    "    model1.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "#     model1.add(BatchNormalization())\n",
    "    model1.add(Activation(\"relu\"))\n",
    "    model1.add(MaxPool2D())\n",
    "    model1.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "#     model1.add(BatchNormalization())\n",
    "    model1.add(Activation(\"relu\"))\n",
    "    model1.add(MaxPool2D())\n",
    "    model1.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "#     model1.add(BatchNormalization())\n",
    "    model1.add(Activation(\"relu\"))\n",
    "    model1.add(MaxPool2D())\n",
    "    model1.add(Flatten())\n",
    "    \n",
    "    # try changing dense layer number, filter_sizes, layers add or subract, change activation\n",
    "    \n",
    "    dense_1 = Dense(100, activation=\"relu\")(model1.output)\n",
    "    dense_2 = Dense(10, activation=\"relu\",name= \"Hg_porosity_diff\")(dense_1)\n",
    "    lambda_layer = Lambda(Cumulative_Sum,name= \"Hg_porosity\")(dense_2)    \n",
    "    big_model = Model([model1.input],[lambda_layer])\n",
    "\n",
    "    big_model.compile(\"adam\",loss=\"mean_squared_error\", metrics = [\"mae\"])\n",
    "    return big_model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator function for slicing on train, validation and test\n",
    "def ImageDataGenerator(X_Train, Y_Train, df_train, img_diam = 224, batch_size = 100, num_batches = 100, slice_per_image = 2, seed = None):\n",
    "    \n",
    "    def slice_image(img, top, bottom, diam, seed = None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)  \n",
    "\n",
    "        rand_row = np.random.randint(top, bottom - diam)\n",
    "        rand_col = np.random.randint(0, img.shape[1] - diam)\n",
    "\n",
    "        return img[rand_row:rand_row + diam, rand_col:rand_col + diam]\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)  \n",
    "        \n",
    "    tot_num_images = len(X_Train) \n",
    "    batch_iteration_list = [np.random.choice(range(tot_num_images), batch_size, replace=True)\n",
    "                            for _ in range(num_batches)]\n",
    "      \n",
    "    while True: # Loop over epochs         \n",
    "        for batch_index in range(num_batches):  # Loop over batches within one epoch\n",
    "            slice_batch = []\n",
    "            hg_porosity_batch = []\n",
    "            rand_list = batch_iteration_list[batch_index]\n",
    "            \n",
    "            i = 0\n",
    "            while i < batch_size:  # Loop over one batch\n",
    "                rand_index = rand_list[i]\n",
    "                top = df_train.iloc[rand_index]['Top_Boundary']\n",
    "                bottom = df_train.iloc[rand_index]['Bottom_Boundary']\n",
    "                \n",
    "                for _ in range(slice_per_image):   # Loop over one image\n",
    "                    img_slice = slice_image(X_Train[rand_index], top, bottom, img_diam)\n",
    "                    \n",
    "                    # add mirroring (append 2) or probaility based image addition\n",
    "                    \n",
    "                    # can add interface slice image here\n",
    "                    # image_interface_slice = slice_interface_image_X_Train(X_interface_Train[rand_index], top, bottom, img_interface_diam)\n",
    "                    # image_interface_slice_norm = image_interface_slice/255\n",
    "                    # slice_batch.append([img_slice_norm, image_interface_slice_norm])\n",
    "                    \n",
    "                    img_slice_norm = img_slice/255\n",
    "                    \n",
    "                    \n",
    "                    # image_slice_norm = vgg19.predict\n",
    "                    \n",
    "                    slice_batch.append(img_slice_norm)\n",
    "                    hg_porosity_batch.append(Y_Train[rand_index])\n",
    "                    \n",
    "                i = i + 1 \n",
    "            \n",
    "            # slice_batch = vgg19.predict\n",
    "            \n",
    "            yield np.array(slice_batch), np.array(hg_porosity_batch)\n",
    "            # yield continues the operation different from conventional function, test will not collide with train data (atleast low probability to) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# both 500 and 2000 threshold images to optimisie number of Y coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 point model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for complete operaion\n",
    "df = df_files[['Sample', 'Image_Location_PostPocess','Image_Location_Threshold','Top_Boundary','Bottom_Boundary']].merge(df_Hg_porosity_5, \n",
    "                                                                                  left_on='Sample', right_on='Sample')\n",
    "Hg = (np.array(df[\"Ps_new\"].tolist())) / 100.\n",
    "df_train, df_holdout, Y_Train, Y_holdout = train_test_split(df, Hg, shuffle = True, random_state = 12, test_size = 0.1)\n",
    "thresh_loc_train = np.array(df_train[\"Image_Location_Threshold\"].tolist(), dtype = 'U')\n",
    "thresh_loc_holdout = np.array(df_holdout[\"Image_Location_Threshold\"].tolist(), dtype = 'U')\n",
    "X_Train = []\n",
    "X_Holdout = []\n",
    "for loc in thresh_loc_train:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    X_Train.append(img)\n",
    "    \n",
    "for loc in thresh_loc_holdout:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    X_Holdout.append(img)\n",
    "    \n",
    "X_Holdout = np.array(X_Holdout)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model created\n",
    "# image_only_model_5Hg = build_image_only_model(nfilter = 16, filter_size=3)\n",
    "# plot_model(image_only_model, \"image_only_model.png\",show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data generated\n",
    "num = 10\n",
    "test_generator = ImageDataGenerator(X_Holdout, Y_holdout, df_holdout, \n",
    "                         img_diam = 224, batch_size = 1, num_batches = num, \n",
    "                         slice_per_image = 1, seed = None)\n",
    "\n",
    "test_data = [result for i, result in zip(range(num), test_generator)]\n",
    "\n",
    "test_images = []\n",
    "test_poros = []\n",
    "for img, poros in (test_data):\n",
    "    test_images.append(img)\n",
    "    test_poros.append(poros)\n",
    "    \n",
    "test_images = np.array(test_images)\n",
    "test_poros = np.array(test_poros)\n",
    "test_images = test_images.reshape(len(test_images),test_images.shape[-2],test_images.shape[-1])\n",
    "test_poros = test_poros.reshape(len(test_images),test_poros.shape[-1])\n",
    "\n",
    "print(\"test_images.shape = \",test_images.shape, \"test_poros.shape = \",test_poros.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model = os.path.join(path_Models, 'image_only_model_5Hg.h5')\n",
    "if not os.path.isfile(file_model):\n",
    "    open(file_model, 'a').close()\n",
    "    \n",
    "checkpoint = ModelCheckpoint(file_model, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "callbacks_control = keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint, callbacks_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the number of epochs and batch size\n",
    "BS = 25\n",
    "EPOCHS = 100\n",
    "VS = 0.15\n",
    "\n",
    "# construct the training image generator for data augmentation\n",
    "train_generator = ImageDataGenerator(X_Train, Y_Train, df_train, \n",
    "                         img_diam = 224, batch_size = 20, num_batches = BS, \n",
    "                         slice_per_image = 10, seed = None)\n",
    "\n",
    "validation_generator = ImageDataGenerator(X_Train, Y_Train, df_train, \n",
    "                         img_diam = 224, batch_size = 10, num_batches = 5, \n",
    "                         slice_per_image = 2, seed = None)\n",
    "\n",
    "history_thresh_5Hg_1 = image_only_model_5Hg.fit_generator(train_generator,validation_data = validation_generator, validation_steps = 5,\n",
    "                        steps_per_epoch = BS, epochs=EPOCHS, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history_thresh_5Hg_1.history[\"loss\"],label=\"train\")\n",
    "# plt.plot(history_thresh_5Hg_1.history[\"val_loss\"],label=\"validation\")\n",
    "# plt.legend()\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = image_only_model_5Hg.predict(test_images)\n",
    "# times = 0\n",
    "# for y_pred_i, y_true_i in zip(y_pred, Y_holdout):\n",
    "#     times = times +1 \n",
    "#     plt.plot(y_pred_i, label=\"pred\")\n",
    "#     plt.plot(y_true_i,label=\"True\")\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 point model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for complete operaion\n",
    "df = df_files[['Sample', 'Image_Location_PostPocess','Image_Location_Threshold','Top_Boundary','Bottom_Boundary']].merge(df_Hg_porosity_10, \n",
    "                                                                                  left_on='Sample', right_on='Sample')\n",
    "Hg = (np.array(df[\"Ps_new\"].tolist())) / 100.\n",
    "df_train, df_holdout, Y_Train, Y_holdout = train_test_split(df, Hg, shuffle = True, random_state = 12, test_size = 0.1)\n",
    "thresh_loc_train = np.array(df_train[\"Image_Location_Threshold\"].tolist(), dtype = 'U')\n",
    "thresh_loc_holdout = np.array(df_holdout[\"Image_Location_Threshold\"].tolist(), dtype = 'U')\n",
    "X_Train = []\n",
    "X_Holdout = []\n",
    "for loc in thresh_loc_train:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    X_Train.append(img)\n",
    "    \n",
    "for loc in thresh_loc_holdout:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    X_Holdout.append(img)\n",
    "    \n",
    "X_Holdout = np.array(X_Holdout)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_only_model_10Hg = build_image_only_model(nfilter = 16, filter_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data generated\n",
    "num = 10\n",
    "test_generator = ImageDataGenerator(X_Holdout, Y_holdout, df_holdout, \n",
    "                         img_diam = 224, batch_size = 1, num_batches = num, \n",
    "                         slice_per_image = 1, seed = None)\n",
    "\n",
    "test_data = [result for i, result in zip(range(num), test_generator)]\n",
    "\n",
    "test_images = []\n",
    "test_poros = []\n",
    "for img, poros in (test_data):\n",
    "    test_images.append(img)\n",
    "    test_poros.append(poros)\n",
    "    \n",
    "test_images = np.array(test_images)\n",
    "test_poros = np.array(test_poros)\n",
    "test_images = test_images.reshape(len(test_images),test_images.shape[-2],test_images.shape[-1])\n",
    "test_poros = test_poros.reshape(len(test_images),test_poros.shape[-1])\n",
    "print(\"test_images.shape = \",test_images.shape, \"test_poros.shape = \",test_poros.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model = os.path.join(path_Models, 'image_only_model_10Hg.h5')\n",
    "if not os.path.isfile(file_model):\n",
    "    open(file_model, 'a').close()\n",
    "    \n",
    "checkpoint = ModelCheckpoint(file_model, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "callbacks_control = keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint, callbacks_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the number of epochs and batch size\n",
    "BS = 25\n",
    "EPOCHS = 100\n",
    "VS = 0.15\n",
    "\n",
    "# construct the training image generator for data augmentation\n",
    "train_generator = ImageDataGenerator(X_Train, Y_Train, df_train, \n",
    "                         img_diam = 224, batch_size = 20, num_batches = BS, \n",
    "                         slice_per_image = 5, seed = None)\n",
    "\n",
    "validation_generator = ImageDataGenerator(X_Train, Y_Train, df_train, \n",
    "                         img_diam = 224, batch_size = 10, num_batches = 5, \n",
    "                         slice_per_image = 2, seed = None)\n",
    "\n",
    "history_thresh_10points = image_only_model_10Hg.fit_generator(train_generator,validation_data = validation_generator, validation_steps = 5,\n",
    "                        steps_per_epoch = BS, epochs=EPOCHS, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "# plt.plot(history_thresh2.history[\"loss\"],label=\"Train\", linewidth=2)\n",
    "# plt.plot(history_thresh2.history[\"val_loss\"],label=\"Validation\", linewidth=2)\n",
    "# plt.legend(fontsize=18)\n",
    "# plt.ylabel(\"MSE Loss\", fontsize=18)\n",
    "# plt.xlabel(\"Epochs\", fontsize=18)\n",
    "# plt.savefig(\"10FittingPointsModel1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = image_only_model_10Hg.predict(test_images)\n",
    "numbers = 0\n",
    "plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "for y_pred_i, y_true_i in zip(y_pred, Y_holdout):\n",
    "    numbers = numbers +1 \n",
    "    if numbers < 3:\n",
    "        plt.plot(y_pred_i, label=\"Predicted Curve \" + str(numbers))\n",
    "        plt.plot(y_true_i,label=\"Original Curve \"+ str(numbers))\n",
    "        plt.ylabel(\"Porosity ratio\", fontsize=18)\n",
    "        plt.xlabel(\"log(Radii)\", fontsize=18)\n",
    "        plt.legend(fontsize=18)\n",
    "    \n",
    "plt.savefig(r\"C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\Files_Save_Backup\\Models_documentation\\10PointsModel\\10FittingPointsModel2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 point model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for complete operaion\n",
    "df = df_files[['Sample', 'Image_Location_PostPocess','Image_Location_Threshold','Top_Boundary','Bottom_Boundary']].merge(df_Hg_porosity_15, \n",
    "                                                                                  left_on='Sample', right_on='Sample')\n",
    "Hg = (np.array(df[\"Ps_new\"].tolist())) / 100.\n",
    "df_train, df_holdout, Y_Train, Y_holdout = train_test_split(df, Hg, shuffle = True, random_state = 12, test_size = 0.1)\n",
    "thresh_loc_train = np.array(df_train[\"Image_Location_Threshold\"].tolist(), dtype = 'U')\n",
    "thresh_loc_holdout = np.array(df_holdout[\"Image_Location_Threshold\"].tolist(), dtype = 'U')\n",
    "X_Train = []\n",
    "X_Holdout = []\n",
    "for loc in thresh_loc_train:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    X_Train.append(img)\n",
    "    \n",
    "for loc in thresh_loc_holdout:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    X_Holdout.append(img)\n",
    "    \n",
    "X_Holdout = np.array(X_Holdout)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_only_model_15Hg = build_image_only_model(nfilter = 16, filter_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data generated\n",
    "num = 10\n",
    "test_generator = ImageDataGenerator(X_Holdout, Y_holdout, df_holdout, \n",
    "                         img_diam = 224, batch_size = 1, num_batches = num, \n",
    "                         slice_per_image = 1, seed = None)\n",
    "\n",
    "test_data = [result for i, result in zip(range(num), test_generator)]\n",
    "\n",
    "test_images = []\n",
    "test_poros = []\n",
    "for img, poros in (test_data):\n",
    "    test_images.append(img)\n",
    "    test_poros.append(poros)\n",
    "    \n",
    "test_images = np.array(test_images)\n",
    "test_poros = np.array(test_poros)\n",
    "test_images = test_images.reshape(len(test_images),test_images.shape[-2],test_images.shape[-1])\n",
    "test_poros = test_poros.reshape(len(test_images),test_poros.shape[-1])\n",
    "print(\"test_images.shape = \",test_images.shape, \"test_poros.shape = \",test_poros.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model = os.path.join(path_Models, 'image_only_model_15Hg.h5')\n",
    "if not os.path.isfile(file_model):\n",
    "    open(file_model, 'a').close()\n",
    "    \n",
    "checkpoint = ModelCheckpoint(file_model, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "callbacks_control = keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint, callbacks_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the number of epochs and batch size\n",
    "BS = 25\n",
    "EPOCHS = 100\n",
    "VS = 0.15\n",
    "\n",
    "# construct the training image generator for data augmentation\n",
    "train_generator = ImageDataGenerator(X_Train, Y_Train, df_train, \n",
    "                         img_diam = 224, batch_size = 20, num_batches = BS, \n",
    "                         slice_per_image = 10, seed = None)\n",
    "\n",
    "validation_generator = ImageDataGenerator(X_Train, Y_Train, df_train, \n",
    "                         img_diam = 224, batch_size = 10, num_batches = 5, \n",
    "                         slice_per_image = 2, seed = None)\n",
    "\n",
    "history_thresh3_2 = image_only_model_15Hg.fit_generator(train_generator,validation_data = validation_generator, validation_steps = 5,\n",
    "                        steps_per_epoch = BS,\n",
    "                        epochs=EPOCHS, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history_thresh3 is old one history_thresh3_2 is the contniued one \n",
    "\n",
    "# plt.plot(history_thresh3.history[\"loss\"],label=\"train\")\n",
    "# plt.plot(history_thresh3.history[\"val_loss\"],label=\"validation\")\n",
    "# plt.legend()\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = image_only_model_15Hg.predict(test_images)\n",
    "# times = 0\n",
    "# for y_pred_i, y_true_i in zip(y_pred, Y_holdout):\n",
    "#     times = times +1 \n",
    "#     if times<2:\n",
    "#         plt.plot(y_pred_i, label=\"pred\")\n",
    "#     #     plt.plot(y_true_i,label=\"True\")\n",
    "#         plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Designed (must be changed for each of the following models)\n",
    "def build_image_only_model(nfilter = 16, filter_size = 3):   \n",
    "    model1 = Sequential()\n",
    "    model1.add(InputLayer(input_shape=(224,224), name= \"bulk_img_input\"))\n",
    "    model1.add(Reshape((224,224,1)))\n",
    "    model1.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\", activation=\"relu\"))\n",
    "#     model1.add(BatchNormalization()) # activation should follow batch normalisation\n",
    "#     model1.add(Activation(\"relu\"))\n",
    "    model1.add(MaxPool2D())\n",
    "    model1.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\", activation=\"relu\"))\n",
    "#     model1.add(BatchNormalization()) # activation should follow batch normalisation\n",
    "#     model1.add(Activation(\"relu\"))\n",
    "    model1.add(MaxPool2D())\n",
    "    model1.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\", activation=\"relu\"))\n",
    "#     model1.add(BatchNormalization()) # activation should follow batch normalisation\n",
    "#     model1.add(Activation(\"relu\"))\n",
    "    model1.add(MaxPool2D())\n",
    "    model1.add(Flatten())\n",
    "    \n",
    "    # try changing dense layer number, filter_sizes, layers add or subract, change activation\n",
    "    \n",
    "    dense_1 = Dense(100, activation=\"relu\")(model1.output)\n",
    "    dense_2 = Dense(10, activation=\"relu\",name= \"Hg_porosity_diff\")(dense_1)\n",
    "    lambda_layer = Lambda(Cumulative_Sum, name= \"Hg_porosity\")(dense_2)    \n",
    "    big_model = Model([model1.input],[lambda_layer])\n",
    "\n",
    "    big_model.compile(\"adam\",loss=\"mean_squared_error\", metrics = [\"mae\"])\n",
    "    return big_model   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 500X Images Alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_500 = (df_files.loc[df_files['Magnification'] == 500.0])[['Sample','Image_Location_PostPocess','Image_Location_Threshold','Top_Boundary','Bottom_Boundary']]\n",
    "df_500 = df_500[['Sample', 'Image_Location_PostPocess','Image_Location_Threshold','Top_Boundary','Bottom_Boundary']].merge(df_Hg_porosity_10, \n",
    "                                                                                  left_on='Sample', right_on='Sample')\n",
    "\n",
    "df_500 = df_500.reset_index()\n",
    "Hg_500 = (np.array(df_500[\"Ps_new\"].tolist())) / 100.\n",
    "\n",
    "df_500_train, df_500_holdout, Y_Train_Hg_500, Y_Train_Hg_500_holdout = train_test_split(df_500, Hg_500, shuffle = True, random_state = 12, test_size = 0.1)\n",
    "thresh_loc_train = np.array(df_500_train[\"Image_Location_Threshold\"].tolist(), dtype = 'U')\n",
    "grey_loc_train = np.array(df_500_train[\"Image_Location_PostPocess\"].tolist(), dtype = 'U')\n",
    "thresh_loc_holdout = np.array(df_500_holdout[\"Image_Location_Threshold\"].tolist(), dtype = 'U')\n",
    "grey_loc_holdout = np.array(df_500_holdout[\"Image_Location_PostPocess\"].tolist(), dtype = 'U')\n",
    "\n",
    "X_Train_thresh_500_train = []\n",
    "X_Train_grey_500_train = []\n",
    "\n",
    "X_Train_thresh_500_holdout = []\n",
    "X_Train_grey_500_holdout = []\n",
    "\n",
    "for loc in thresh_loc_train:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    X_Train_thresh_500_train.append(img)\n",
    "    \n",
    "for loc in grey_loc_train:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    X_Train_grey_500_train.append(img)\n",
    "    \n",
    "for loc in thresh_loc_holdout:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    X_Train_thresh_500_holdout.append(img)\n",
    "    \n",
    "for loc in grey_loc_holdout:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    X_Train_grey_500_holdout.append(img)\n",
    "    \n",
    "X_Train_thresh_500_holdout = np.array(X_Train_thresh_500_holdout)/255\n",
    "X_Train_grey_500_holdout = np.array(X_Train_grey_500_holdout)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_only_model_500X_threshold = build_image_only_model(nfilter = 16, filter_size=3) # had all layers\n",
    "# image_only_model_500X_threshold2 = build_image_only_model(nfilter = 32, filter_size=3) # had only one layer \n",
    "# image_only_model_500X_threshold3 = build_image_only_model(nfilter = 16, filter_size=3) # softplus for input, tanh for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data generated\n",
    "num = 10\n",
    "test_generator = ImageDataGenerator(X_Train_thresh_500_holdout, Y_Train_Hg_500_holdout, df_500_holdout, \n",
    "                         img_diam = 224, batch_size = 1, num_batches = num, \n",
    "                         slice_per_image = 1, seed = None)\n",
    "\n",
    "test_data = [result for i, result in zip(range(num), test_generator)]\n",
    "\n",
    "test_images = []\n",
    "test_poros = []\n",
    "for img, poros in (test_data):\n",
    "    test_images.append(img)\n",
    "    test_poros.append(poros)\n",
    "    \n",
    "test_images = np.array(test_images)\n",
    "test_poros = np.array(test_poros)\n",
    "test_images = test_images.reshape(len(test_images),test_images.shape[-2],test_images.shape[-1])\n",
    "test_poros = test_poros.reshape(len(test_images),test_poros.shape[-1])\n",
    "print(\"test_images.shape = \",test_images.shape, \"test_poros.shape = \",test_poros.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model = os.path.join(path_Models, 'image_only_model_500X_threshold3.h5')\n",
    "if not os.path.isfile(file_model):\n",
    "    open(file_model, 'a').close()\n",
    "    \n",
    "checkpoint = ModelCheckpoint(file_model, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "callbacks_control = keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint, callbacks_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the number of epochs and batch size\n",
    "BS = 25\n",
    "EPOCHS = 100\n",
    "VS = 5\n",
    "\n",
    "# construct the training image generator for data augmentation\n",
    "train_generator = ImageDataGenerator(X_Train_thresh_500_train, Y_Train_Hg_500, df_500_train, \n",
    "                         img_diam = 224, batch_size = 20, num_batches = BS, \n",
    "                         slice_per_image = 5, seed = None)\n",
    "\n",
    "validation_generator = ImageDataGenerator(X_Train_thresh_500_train, Y_Train_Hg_500, df_500_train, \n",
    "                         img_diam = 224, batch_size = 10, num_batches = VS, \n",
    "                         slice_per_image = 2, seed = None)\n",
    "\n",
    "#history_image_only_model_500X_threshold tested with only straight lines as output\n",
    "history_image_only_model_500X_threshold3 = image_only_model_500X_threshold3.fit_generator(train_generator, validation_data = validation_generator, validation_steps = VS,\n",
    "                        steps_per_epoch = BS, epochs=EPOCHS, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "# plt.plot(history_image_only_model_500X_threshold3.history[\"loss\"],label=\"Train\", linewidth=2)\n",
    "# plt.plot(history_image_only_model_500X_threshold3.history[\"val_loss\"],label=\"Validation\", linewidth=2)\n",
    "# plt.legend(fontsize=18)\n",
    "# plt.ylabel(\"MSE Loss\", fontsize=18)\n",
    "# plt.xlabel(\"Epochs\", fontsize=18)\n",
    "# # plt.savefig(\"image_only_model_500X1.png\")\n",
    "\n",
    "# print(signaltonoise(history_image_only_model_500X_threshold3.history[\"val_loss\"]))\n",
    "# print(signaltonoise(history_image_only_model_500X_threshold3.history[\"val_loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = image_only_model_500X_threshold3.predict(test_images)\n",
    "numbers = 0\n",
    "plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "for y_pred_i, y_true_i in zip(y_pred, Y_holdout):\n",
    "    numbers = numbers +1\n",
    "    if numbers < 10:\n",
    "        plt.plot(y_pred_i, label=\"Predicted Curve \" + str(numbers))\n",
    "#         plt.plot(y_true_i,label=\"Original Curve \"+ str(numbers))\n",
    "#         plt.ylabel(\"Porosity ratio\", fontsize=18)\n",
    "        plt.xlabel(\"log(Radii)\", fontsize=18)\n",
    "#         plt.legend(fontsize=18)\n",
    "        \n",
    "# plt.savefig(\"image_only_model_500X2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2000X images alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we get all the currosponding images...join with their modified porosimetry curves..then take out the Y part which has the same size\n",
    "df_2000 = (df_files.loc[df_files['Magnification'] == 2000.0])[['Sample','Image_Location_PostPocess','Image_Location_Threshold','Top_Boundary','Bottom_Boundary']]\n",
    "df_2000 = df_2000[['Sample', 'Image_Location_PostPocess','Image_Location_Threshold','Top_Boundary','Bottom_Boundary']].merge(df_Hg_porosity_10, \n",
    "                                                                                  left_on='Sample', right_on='Sample')\n",
    "df_2000 = df_2000.reset_index()\n",
    "Hg_2000 = (np.array(df_2000[\"Ps_new\"].tolist())) / 100.\n",
    "#******************************************************\n",
    "\n",
    "df_2000_train, df_2000_holdout, Y_Train_Hg_2000, Y_Train_Hg_2000_holdout = train_test_split(df_2000, Hg_2000, shuffle = True, random_state = 12, test_size = 0.1)\n",
    "thresh_loc_train = np.array(df_2000_train[\"Image_Location_Threshold\"].tolist(), dtype = 'U')\n",
    "grey_loc_train = np.array(df_2000_train[\"Image_Location_PostPocess\"].tolist(), dtype = 'U')\n",
    "thresh_loc_holdout = np.array(df_2000_holdout[\"Image_Location_Threshold\"].tolist(), dtype = 'U')\n",
    "grey_loc_holdout = np.array(df_2000_holdout[\"Image_Location_PostPocess\"].tolist(), dtype = 'U')\n",
    "#******************************************************\n",
    "\n",
    "# Training and holdout/test data\n",
    "X_Train_thresh_2000_train = []\n",
    "X_Train_grey_2000_train = []\n",
    "\n",
    "X_Train_thresh_2000_holdout = []\n",
    "X_Train_grey_2000_holdout = []\n",
    "\n",
    "for loc in thresh_loc_train:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    X_Train_thresh_2000_train.append(img)\n",
    "    \n",
    "for loc in grey_loc_train:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    X_Train_grey_2000_train.append(img)\n",
    "    \n",
    "for loc in thresh_loc_holdout:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    X_Train_thresh_2000_holdout.append(img)\n",
    "    \n",
    "for loc in grey_loc_holdout:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    X_Train_grey_2000_holdout.append(img)\n",
    "    \n",
    "# normalising holdout\n",
    "X_Train_thresh_2000_holdout = np.array(X_Train_thresh_2000_holdout)/255\n",
    "X_Train_grey_2000_holdout = np.array(X_Train_grey_2000_holdout)/255\n",
    "print(\"Completed data extraction successfully..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_only_model_2000X_threshold = build_image_only_model(nfilter = 16, filter_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data generated\n",
    "test_num = 10\n",
    "test_generator = ImageDataGenerator(X_Train_thresh_2000_holdout, Y_Train_Hg_2000_holdout, df_2000_holdout, \n",
    "                         img_diam = 224, batch_size = 1, num_batches = test_num, \n",
    "                         slice_per_image = 1, seed = None)\n",
    "test_data = [result for i, result in zip(range(num), test_generator)]\n",
    "test_images = []\n",
    "test_poros = []\n",
    "for img, poros in (test_data):\n",
    "    test_images.append(img)\n",
    "    test_poros.append(poros)\n",
    "    \n",
    "test_images = np.array(test_images)\n",
    "test_poros = np.array(test_poros)\n",
    "test_images = test_images.reshape(len(test_images),test_images.shape[-2],test_images.shape[-1])\n",
    "test_poros = test_poros.reshape(len(test_images),test_poros.shape[-1])\n",
    "print(\"test_images.shape = \",test_images.shape, \"test_poros.shape = \",test_poros.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model = os.path.join(path_Models, 'image_only_model_2000X_threshold.h5')\n",
    "if not os.path.isfile(file_model):\n",
    "    open(file_model, 'a').close()\n",
    "    \n",
    "checkpoint = ModelCheckpoint(file_model, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "callbacks_control = keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint, callbacks_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the number of epochs and batch size\n",
    "BS = 25\n",
    "EPOCHS = 100\n",
    "VS = 0.15\n",
    "\n",
    "# construct the training image generator for data augmentation\n",
    "train_generator = ImageDataGenerator(X_Train_thresh_2000_train, Y_Train_Hg_2000, df_2000_train, \n",
    "                         img_diam = 224, batch_size = 20, num_batches = BS, \n",
    "                         slice_per_image = 5, seed = None)\n",
    "\n",
    "validation_generator = ImageDataGenerator(X_Train_thresh_2000_train, Y_Train_Hg_2000, df_2000_train,\n",
    "                         img_diam = 224, batch_size = 10, num_batches = 5, \n",
    "                         slice_per_image = 2, seed = None)\n",
    "\n",
    "history_image_only_model_2000X_threshold = image_only_model_2000X_threshold.fit_generator(train_generator, validation_data = validation_generator, validation_steps = VS,\n",
    "                        steps_per_epoch = BS, epochs=EPOCHS, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "# plt.plot(history_image_only_model_2000X_threshold.history[\"mae\"],label=\"Training Error\", linewidth=2)\n",
    "# plt.plot(history_image_only_model_2000X_threshold.history[\"val_mae\"],label=\"Validation Error\", linewidth=2, color = 'g')\n",
    "# plt.legend(fontsize=18)\n",
    "# plt.ylabel(\"MAE Loss\", fontsize=18)\n",
    "# plt.xlabel(\"Epochs\", fontsize=18)\n",
    "# plt.savefig(\"history_image_only_model_2000X_threshold_MAE.png\")\n",
    "\n",
    "# print(signaltonoise(history_image_only_model_2000X_threshold.history[\"val_loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = image_only_model_2000X_threshold.predict(test_images)\n",
    "numbers = 0\n",
    "plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "for y_pred_i, y_true_i in zip(y_pred, Y_holdout):\n",
    "    numbers = numbers +1 \n",
    "    if numbers <2:\n",
    "        plt.plot(y_pred_i, label=\"Predicted Curve \" + str(numbers))\n",
    "#         plt.plot(y_true_i,label=\"Original Curve \"+ str(numbers))\n",
    "        plt.ylabel(\"Porosity ratio\", fontsize=18)\n",
    "        plt.xlabel(\"log(Radii)\", fontsize=18)\n",
    "#         plt.legend(fontsize=18)\n",
    "# # plt.savefig(\"image_only_model_2000X_cumulative2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2000X images with greyscale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_only_model_2000X_greyscale = build_image_only_model(nfilter = 16, filter_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data generated\n",
    "num = 10\n",
    "test_generator = ImageDataGenerator(X_Train_grey_2000_holdout, Y_Train_Hg_2000_holdout, df_2000_holdout, \n",
    "                         img_diam = 224, batch_size = 1, num_batches = num, \n",
    "                         slice_per_image = 1, seed = None)\n",
    "\n",
    "test_data = [result for i, result in zip(range(num), test_generator)]\n",
    "\n",
    "test_images = []\n",
    "test_poros = []\n",
    "for img, poros in (test_data):\n",
    "    test_images.append(img)\n",
    "    test_poros.append(poros)\n",
    "    \n",
    "test_images = np.array(test_images)\n",
    "test_poros = np.array(test_poros)\n",
    "test_images = test_images.reshape(len(test_images),test_images.shape[-2],test_images.shape[-1])\n",
    "test_poros = test_poros.reshape(len(test_images),test_poros.shape[-1])\n",
    "print(\"test_images.shape = \",test_images.shape, \"test_poros.shape = \",test_poros.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model = os.path.join(path_Models, 'image_only_model_2000X_greyscale.h5')\n",
    "if not os.path.isfile(file_model):\n",
    "    open(file_model, 'a').close()\n",
    "    \n",
    "checkpoint = ModelCheckpoint(file_model, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "callbacks_control = keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=25, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint, callbacks_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the number of epochs and batch size\n",
    "BS = 25\n",
    "EPOCHS = 100\n",
    "VS = 0.15\n",
    "\n",
    "# construct the training image generator for data augmentation\n",
    "train_generator = ImageDataGenerator(X_Train_grey_2000_train, Y_Train_Hg_2000, df_2000_train, \n",
    "                         img_diam = 224, batch_size = 20, num_batches = BS, \n",
    "                         slice_per_image = 10, seed = None)\n",
    "\n",
    "validation_generator = ImageDataGenerator(X_Train_grey_2000_train, Y_Train_Hg_2000, df_2000_train, \n",
    "                         img_diam = 224, batch_size = 10, num_batches = 5, \n",
    "                         slice_per_image = 2, seed = None)\n",
    "\n",
    "history_image_only_model_2000X_greyscale = image_only_model_2000X_greyscale.fit_generator(train_generator, validation_data = validation_generator, validation_steps = VS,\n",
    "                        steps_per_epoch = BS, epochs=EPOCHS, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(history_image_only_model_2000X_greyscale.history[\"loss\"],label=\"Train\", linewidth=2)\n",
    "plt.plot(history_image_only_model_2000X_greyscale.history[\"val_loss\"],label=\"Validation\", linewidth=2)\n",
    "plt.legend(fontsize=18)\n",
    "plt.ylabel(\"MSE Loss\", fontsize=18)\n",
    "plt.xlabel(\"Epochs\", fontsize=18)\n",
    "# plt.savefig(\"image_only_model_500X1.png\")\n",
    "\n",
    "print(signaltonoise(history_image_only_model_2000X_greyscale.history[\"val_loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = image_only_model_2000X_greyscale.predict(test_images)\n",
    "numbers = 0\n",
    "plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "for y_pred_i, y_true_i in zip(y_pred, Y_Train_Hg_2000_holdout):\n",
    "    numbers = numbers +1\n",
    "    if numbers < 10:\n",
    "#         plt.plot(y_pred_i, label=\"Predicted Curve using 2000X greyscale image\" + str(numbers), linewidth=5.0, color = 'r')\n",
    "        plt.plot(y_true_i,label=\"Original Curve \"+ str(numbers), linewidth=2.0, color = 'g')\n",
    "        plt.ylabel(\"Porosity ratio\", fontsize=18)\n",
    "        plt.xlabel(\"log(Radii)\", fontsize=18)\n",
    "        plt.ylim(top = 0.22, bottom = 0)\n",
    "#         plt.legend(fontsize=18)\n",
    "        \n",
    "plt.savefig(r\"C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\Files_Save_Backup\\Models_documentation\\2000Xgrey500Xgrey_predictions\\image_only_model_Original.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1 = y_pred[0]\n",
    "y1 = Y_Train_Hg_2000_holdout[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 500X images with greyscale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_only_model_500X_greyscale = build_image_only_model(nfilter = 16, filter_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data generated\n",
    "num = 10\n",
    "test_generator = ImageDataGenerator(X_Train_grey_500_holdout, Y_Train_Hg_500_holdout, df_500_holdout, \n",
    "                         img_diam = 224, batch_size = 1, num_batches = num, \n",
    "                         slice_per_image = 1, seed = None)\n",
    "\n",
    "test_data = [result for i, result in zip(range(num), test_generator)]\n",
    "\n",
    "test_images = []\n",
    "test_poros = []\n",
    "for img, poros in (test_data):\n",
    "    test_images.append(img)\n",
    "    test_poros.append(poros)\n",
    "    \n",
    "test_images = np.array(test_images)\n",
    "test_poros = np.array(test_poros)\n",
    "test_images = test_images.reshape(len(test_images),test_images.shape[-2],test_images.shape[-1])\n",
    "test_poros = test_poros.reshape(len(test_images),test_poros.shape[-1])\n",
    "print(\"test_images.shape = \",test_images.shape, \"test_poros.shape = \",test_poros.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_model = os.path.join(path_Models, 'image_only_model_500X_greyscale.h5')\n",
    "if not os.path.isfile(file_model):\n",
    "    open(file_model, 'a').close()\n",
    "    \n",
    "checkpoint = ModelCheckpoint(file_model, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "callbacks_control = keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=25, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint, callbacks_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the number of epochs and batch size\n",
    "BS = 25\n",
    "EPOCHS = 100\n",
    "VS = 0.15\n",
    "\n",
    "# construct the training image generator for data augmentation\n",
    "train_generator = ImageDataGenerator(X_Train_grey_500_train, Y_Train_Hg_500, df_500_train, \n",
    "                         img_diam = 224, batch_size = 20, num_batches = BS, \n",
    "                         slice_per_image = 5, seed = None)\n",
    "\n",
    "validation_generator = ImageDataGenerator(X_Train_grey_500_train, Y_Train_Hg_500, df_500_train, \n",
    "                         img_diam = 224, batch_size = 10, num_batches = 5, \n",
    "                         slice_per_image = 2, seed = None)\n",
    "\n",
    "history_image_only_model_500X_greyscale = image_only_model_500X_greyscale.fit_generator(train_generator, validation_data = validation_generator, validation_steps = VS,\n",
    "                        steps_per_epoch = BS, epochs=EPOCHS, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.plot(history_image_only_model_500X_greyscale.history[\"loss\"],label=\"Train\", linewidth=2)\n",
    "plt.plot(history_image_only_model_500X_greyscale.history[\"val_loss\"],label=\"Validation\", linewidth=2)\n",
    "plt.legend(fontsize=18)\n",
    "plt.ylabel(\"MSE Loss\", fontsize=18)\n",
    "plt.xlabel(\"Epochs\", fontsize=18)\n",
    "# plt.savefig(\"image_only_model_500X1.png\")\n",
    "\n",
    "print(signaltonoise(history_image_only_model_500X_greyscale.history[\"val_loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = image_only_model_500X_greyscale.predict(test_images)\n",
    "numbers = 0\n",
    "plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "for y_pred_i, y_true_i in zip(y_pred, Y_Train_Hg_500_holdout):\n",
    "    numbers = numbers +1\n",
    "    if numbers < 2:\n",
    "        plt.plot(y_pred_i, label=\"Predicted Curve using 500X greyscale image\" + str(numbers), linewidth=5.0)\n",
    "#         plt.plot(y_true_i,label=\"Original Curve \"+ str(numbers))\n",
    "        plt.ylabel(\"Porosity ratio\", fontsize=14)\n",
    "        plt.xlabel(\"log(Radii)\", fontsize=18)\n",
    "        plt.ylim(top = 0.12)\n",
    "        plt.legend(fontsize=18)\n",
    "        \n",
    "# plt.savefig(r\"C:\\Users\\arjun\\Downloads\\Juelich_Thesis\\ML_Final\\Files_Save_Backup\\Models_documentation\\2000Xgrey500Xgrey_predictions\\image_only_model_500X2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x2 = y_pred[0]\n",
    "y2 = Y_Train_Hg_500_holdout[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

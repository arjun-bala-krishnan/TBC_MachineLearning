{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import functions\n",
    "importlib.reload(functions)\n",
    "from functions import *\n",
    "directory_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This snippet widens the jupyter notebook layout for coding\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "# We check and initialize all missing pre-process folder\n",
    "directory_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files and image data extraction\n",
    "df_files = df_files_update(df_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Processing Files\n",
    "df_files = df_files_preprocess(df_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interface Detection and Extraction\n",
    "df_files = df_files_interface(df_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Threshold Detection and Save\n",
    "df_files = df_files_threshold(df_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files_check(df_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_files.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing Operation - Random\n",
    "df_slice = df_slice_update(df_files, df_slice, slice_sq_dim = 224, overlap = \"Yes\", no_of_slices = 20)\n",
    "# df_slice_intermediate = df_slice_file_display()\n",
    "df_slice_check(df_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porosity Extraction\n",
    "df_slice_porosity = df_slice_porosity_update(df_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porosity Updation till the last processed slice\n",
    "df_slice_porosity = df_slice_porosity_file_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slice_porosity.head()\n",
    "df_slice_porosity.Sample.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mercury Porosimetry Data Extraction\n",
    "datas_, Hg_df_dict = df_mercury_data_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in Hg_df_dict.items():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Mercury Porosimetry Data Points to Important Sampling Points\n",
    "Hg_porosity_df = df_mercury_common_cordinates(datas_, Hg_df_dict, df_slice_porosity, num = 10, kind = \"linear\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Hg_porosity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coating parameter extraction\n",
    "df_parameters = df_parameters_data_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coating parameter index segmentation\n",
    "df_slice_parameters = df_parameters_match(df_parameters, df_slice_porosity, bounded = True, max_powder_dia = 200, div = 10)\n",
    "# display(df_slice_parameters.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slice_parameters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coating roughness extraction                   \n",
    "df_roughness = df_roughness_data_extract()\n",
    "# display(df_roughness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roughness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Reshape, BatchNormalization, MaxPool2D, Convolution2D, InputLayer, Flatten, Concatenate, Lambda, Activation\n",
    "from keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Model\n",
    "from keras import backend as K\n",
    "\n",
    "def Cumulative_Sum(inp):\n",
    "    return K.reverse(K.cumsum(inp,axis=1),axes=1)\n",
    "\n",
    "# import tensorflow as tf\n",
    "# config = tf.compat.v1.ConfigProto\n",
    "# session = tf.compat.v1.InteractiveSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2000X Slice Images generated \"on_the_Go\" = 2000_image_only_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataframe of 2000X variables (locatoions) and labels\n",
    "df_files_2000_temp = (df_files.loc[df_files['Magnification'] == 2000.0])[['Sample','Image_Location_PostPocess']]\n",
    "Hg_porosity_df_unique = Hg_porosity_df[['Sample', 'log_Radii_new', 'Ps_new']]\n",
    "\n",
    "Hg_porosity_df_unique = Hg_porosity_df_unique.sort_values('Sample', ascending=True)\n",
    "Hg_porosity_df_unique = Hg_porosity_df_unique.drop_duplicates(subset='Sample', keep='first')\n",
    "Hg_porosity_df_unique = Hg_porosity_df_unique.reset_index(drop=True)\n",
    "df_files_2000 = df_files_2000_temp[['Sample', 'Image_Location_PostPocess']].merge(Hg_porosity_df_unique, \n",
    "                                                                                  left_on='Sample', right_on='Sample')\n",
    "Y_Train_Hg_2000 = np.array(df_files_2000[\"Ps_new\"].tolist())\n",
    "Y_Train_Hg_2000 = Y_Train_Hg_2000 / 100.\n",
    "df_2000_train, df_2000_holdout, Y_Train_Hg_2000_train, Y_Train_Hg_2000_holdout = train_test_split(df_files_2000, Y_Train_Hg_2000, \n",
    "                                                                                                shuffle = True, random_state = 12, test_size = 0.1)\n",
    "X_Train__img_2000_loc_train = np.array(df_2000_train[\"Image_Location_PostPocess\"].tolist(), dtype = 'U')\n",
    "X_Train__img_2000_train = []\n",
    "for loc in X_Train__img_2000_loc_train:\n",
    "    img = cv2.imread(str(loc), cv2.IMREAD_UNCHANGED) \n",
    "    X_Train__img_2000_train.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_2000_image_only_model(nfilter = 16, filter_size = 3):   \n",
    "    model1 = Sequential()\n",
    "    model1.add(InputLayer(input_shape=(224,224), name= \"bulk_img_input_2000\"))\n",
    "    model1.add(Reshape((224,224,1)))\n",
    "    model1.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    model1.add(Activation(\"relu\"))\n",
    "    model1.add(MaxPool2D())\n",
    "    model1.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    model1.add(Activation(\"relu\"))\n",
    "    model1.add(MaxPool2D())\n",
    "    model1.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    model1.add(Activation(\"relu\"))\n",
    "    model1.add(MaxPool2D())\n",
    "    model1.add(Flatten())\n",
    "    \n",
    "    \n",
    "    dense_1 = Dense(100, activation=\"tanh\")(model1.output)\n",
    "    dense_2 = Dense(10, activation=\"relu\",name= \"Hg_porosity_diff\")(dense_1)\n",
    "    lambda_layer = Lambda(Cumulative_Sum,name= \"Hg_porosity\")(dense_2)    \n",
    "    big_model = Model([model1.input],[lambda_layer])\n",
    "\n",
    "    big_model.compile(\"adam\",loss=\"mean_squared_error\")\n",
    "    return big_model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_generator(img_files, Hg_files, img_diam = 224, num_samples = 100, seed = None):\n",
    "    i = 0\n",
    "    tot_num_images = np.shape(img_files)[0]\n",
    "    threshold_image = []\n",
    "    hg_porosity = []\n",
    "    \n",
    "    X_train = []\n",
    "    X_holdout = []\n",
    "    Y_Train = []\n",
    "    Y_holdout = []\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    rand_list = [np.random.randint(0,tot_num_images) for i in range(num_samples)]\n",
    "    # Random.randint\n",
    "    \n",
    "    while i<num_samples:\n",
    "        print(i)\n",
    "        rand_index = rand_list[i]\n",
    "        img = img_to_slices(img_files[rand_index], diam = 224, no_of_slices = 1)\n",
    "        view2_image(img_files[rand_index], img[0])\n",
    "        threshold_image.append(img[0])\n",
    "        hg_porosity.append(Hg_files[rand_index])\n",
    "        i = i + 1\n",
    "        \n",
    "    return threshold_image, hg_porosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_Train = custom_generator(X_Train__img_2000_train, Y_Train_Hg_2000_train,\n",
    "                                                          img_diam = 224, num_samples = 10, seed = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)/255\n",
    "X_holdout = np.array(X_holdout)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_only_model_2000 = build_2000_image_only_model(nfilter = 4, filter_size=3)\n",
    "history_total = image_only_model_2000.fit(x = [X_train], y = [Y_Train], batch_size=20,\n",
    "                                          validation_split=0.15, epochs=50,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_total.history[\"loss\"],label=\"train\")\n",
    "plt.plot(history_total.history[\"val_loss\"],label=\"test\")\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = image_only_model_2000.predict([X_holdout]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_pred_i, y_true_i in zip(Y_pred, Y_holdout):\n",
    "    plt.plot(y_pred_i, label=\"pred\")\n",
    "    plt.plot(y_true_i,label=\"True\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 500X Images and ML combinations using data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.merge(df_slice, df_slice_porosity)\n",
    "Y_Train_Hg_porosity = np.array(Hg_porosity_df[\"Ps_new\"].tolist())\n",
    "Y_Train_Hg_porosity = Y_Train_Hg_porosity / 100.\n",
    "df_total[\"Points_area\"] = df_total[\"Points_area\"]/df_total[\"Total_area\"]  \n",
    "df_total[\"Threads_area\"] = df_total[\"Threads_area\"]/df_total[\"Total_area\"]  \n",
    "df_total[\"Clusters_area\"] = df_total[\"Clusters_area\"]/df_total[\"Total_area\"]  \n",
    "df_total[\"Slice_Array\"] = df_total[\"Slice_Array\"].map(lambda l:np.array(l)/255)\n",
    "df_total[\"Interface_Array\"] = df_total[\"Interface_Array\"].map(lambda l:np.array(l)/255)\n",
    "df_total[\"Slice_Array_Threshold\"] = df_total[\"Slice_Array_Threshold\"].map(lambda l:np.array(l)/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_holdout, Y_Train_Hg_porosity_train, Y_Train_Hg_porosity_holdout = train_test_split(df_total, Y_Train_Hg_porosity, shuffle = True, random_state = 12, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert Y_Train_Hg_porosity.shape[0] == Y_Train_Hg_porosity_train.shape[0] + Y_Train_Hg_porosity_holdout.shape[0]\n",
    "assert df_total.shape[0] ==df_train.shape[0] + df_holdout.shape[0]\n",
    "assert Y_Train_Hg_porosity_train.shape[0] == df_train.shape[0]\n",
    "assert df_holdout.shape[0] == Y_Train_Hg_porosity_holdout.shape[0]\n",
    "\n",
    "X_Train_bulk_img_train = np.array(df_train[\"Slice_Array\"].tolist())\n",
    "X_Train_inter_img_train =  np.array(df_train[\"Interface_Array\"].tolist())\n",
    "X_Train_porosity_train =df_train[[\"Points_area\",\"Threads_area\",\"Clusters_area\"]].values\n",
    "\n",
    "X_Train_bulk_img_holdout = np.array(df_holdout[\"Slice_Array\"].tolist())\n",
    "X_Train_inter_img_hodlout =  np.array(df_holdout[\"Interface_Array\"].tolist())\n",
    "X_Train_porosity_holdout =df_holdout[[\"Points_area\",\"Threads_area\",\"Clusters_area\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_image_only_model(nfilter = 16, filter_size = 3):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(224,224), name=\"bulk_img_input\"))\n",
    "    model.add(Reshape((224,224,1)))\n",
    "    model.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model2 = Sequential()\n",
    "    model2.add(InputLayer(input_shape=(224, 224), name=\"interface_img_input\"))\n",
    "    model2.add(Reshape((224,224,1)))\n",
    "    model2.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model2.add(BatchNormalization())\n",
    "    model2.add(Activation(\"relu\"))\n",
    "    model2.add(MaxPool2D())\n",
    "    \n",
    "    model2.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model2.add(BatchNormalization())\n",
    "    model2.add(Activation(\"relu\"))\n",
    "    model2.add(MaxPool2D())\n",
    "    \n",
    "    model2.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model2.add(BatchNormalization())\n",
    "    model2.add(Activation(\"relu\"))\n",
    "    model2.add(MaxPool2D())\n",
    "    \n",
    "    model2.add(Flatten())\n",
    "   \n",
    "    concat_layer = Concatenate()([model.output,model2.output])\n",
    "\n",
    "    dense_1 = Dense(100, activation=\"tanh\")(concat_layer)\n",
    "    dense_2 = Dense(10, activation=\"relu\",name= \"Hg_porosity_diff\")(dense_1)\n",
    "    lambda_layer = Lambda(Cumulative_Sum,name= \"Hg_porosity\")(dense_2)    \n",
    "    big_model = Model([model.input, model2.input],[lambda_layer])\n",
    "    #big_model = Model([model.input, model2.input],[dense_2])\n",
    "\n",
    "    big_model.compile(\"adam\",loss=\"mean_squared_error\")\n",
    "    return big_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_only_model = build_image_only_model(nfilter = 4, filter_size=3)\n",
    "# image_only_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_image = image_only_model.fit(x=[X_Train_bulk_img_train,X_Train_inter_img_train], \n",
    "                                     y = Y_Train_Hg_porosity_train, batch_size=20,validation_split=0.15, \n",
    "                                     epochs=25,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = image_only_model.predict([X_Train_bulk_img_holdout,X_Train_inter_img_hodlout]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for y_pred_i, y_true_i in zip(y_pred, Y_Train_Hg_porosity_holdout):\n",
    "    plt.plot(y_pred_i, label=\"pred\")\n",
    "    plt.plot(y_true_i,label=\"True\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_image.history[\"loss\"],label=\"train\")\n",
    "plt.plot(history_image.history[\"val_loss\"],label=\"test\")\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(history_image.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slice Images + Interfaces+ Extracted Porosity Training set = Total Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_image_plus_porosity_model():\n",
    "    model1 = Sequential()\n",
    "    model1.add(InputLayer(input_shape=(224,224), name= \"bulk_img_input\"))\n",
    "    model1.add(Reshape((224,224,1)))\n",
    "    model1.add(Convolution2D(16,5,strides=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "    model1.add(Convolution2D(16,5,strides=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "    \n",
    "    model1.add(Convolution2D(16,5,strides=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "    model1.add(Flatten())\n",
    "\n",
    "    model2 = Sequential()\n",
    "    model2.add(InputLayer(input_shape=(224, 224), name=\"interface_img_input\"))\n",
    "    model2.add(Reshape((224,224,1)))\n",
    "    model2.add(Convolution2D(16,5,strides=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "    model2.add(Convolution2D(16,5,strides=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "    model2.add(Convolution2D(16,5,strides=(3,3), padding=\"valid\", activation=\"relu\"))\n",
    "    model2.add(Flatten())\n",
    "\n",
    "    model3 = Sequential()\n",
    "    model3.add(InputLayer(input_shape=(3,), name= \"porosity_input\"))\n",
    "    model3.add(Dense(10, activation=\"relu\"))\n",
    "    \n",
    "    im_concat_layer = Concatenate()([model1.output, model2.output])\n",
    "    dense_por = Dense(5, activation=\"tanh\")(model3.output)\n",
    "    dense_im = Dense(5, activation=\"tanh\")(im_concat_layer)\n",
    "    \n",
    "    concat_layer = Concatenate(name= \"Hg_porosity_diff\")([dense_por, dense_im])\n",
    "    \n",
    "    lambda_layer = Lambda(Cumulative_Sum, name= \"Hg_porosity\")(concat_layer)\n",
    "    \n",
    "    big_model = Model([model1.input, model2.input, model3.input],[lambda_layer])    \n",
    "    big_model.compile(\"adam\",loss=\"mean_squared_error\")\n",
    "    \n",
    "    return big_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_model = build_image_plus_porosity_model()\n",
    "total_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_total = total_model.fit(x=[X_Train_bulk_img_train,X_Train_inter_img_train, X_Train_porosity_train], \n",
    "                        y = Y_Train_Hg_porosity_train, batch_size=25,validation_split=0.15, \n",
    "                        epochs=50,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = total_model.predict([X_Train_bulk_img_holdout, X_Train_inter_img_hodlout, \n",
    "                              X_Train_porosity_holdout])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for y_pred_i, y_true_i in zip(y_pred, Y_Train_Hg_porosity_holdout):\n",
    "    plt.plot(y_pred_i, label=\"pred\")\n",
    "    plt.plot(y_true_i,label=\"True\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history_total.history[\"loss\"],label=\"train\")\n",
    "plt.plot(history_total.history[\"val_loss\"],label=\"test\")\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(history_total.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slice Images + Interface Images ONLY  = Image Only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_image_only_model(nfilter = 16, filter_size = 3):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(224,224), name=\"bulk_img_input\"))\n",
    "    model.add(Reshape((224,224,1)))\n",
    "    model.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model2 = Sequential()\n",
    "    model2.add(InputLayer(input_shape=(224, 224), name=\"interface_img_input\"))\n",
    "    model2.add(Reshape((224,224,1)))\n",
    "    model2.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model2.add(BatchNormalization())\n",
    "    model2.add(Activation(\"relu\"))\n",
    "    model2.add(MaxPool2D())\n",
    "    \n",
    "    model2.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model2.add(BatchNormalization())\n",
    "    model2.add(Activation(\"relu\"))\n",
    "    model2.add(MaxPool2D())\n",
    "    \n",
    "    model2.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model2.add(BatchNormalization())\n",
    "    model2.add(Activation(\"relu\"))\n",
    "    model2.add(MaxPool2D())\n",
    "    \n",
    "    model2.add(Flatten())\n",
    "   \n",
    "    concat_layer = Concatenate()([model.output,model2.output])\n",
    "\n",
    "    dense_1 = Dense(100, activation=\"tanh\")(concat_layer)\n",
    "    dense_2 = Dense(10, activation=\"relu\",name= \"Hg_porosity_diff\")(dense_1)\n",
    "    lambda_layer = Lambda(Cumulative_Sum,name= \"Hg_porosity\")(dense_2)    \n",
    "    big_model = Model([model.input, model2.input],[lambda_layer])\n",
    "    #big_model = Model([model.input, model2.input],[dense_2])\n",
    "\n",
    "    big_model.compile(\"adam\",loss=\"mean_squared_error\")\n",
    "    return big_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_only_model = build_image_only_model(nfilter = 4, filter_size=3)\n",
    "# image_only_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_bulk_img_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_image = image_only_model.fit(x=[X_Train_bulk_img_train,X_Train_inter_img_train], \n",
    "                                     y = Y_Train_Hg_porosity_train, batch_size=20,validation_split=0.15, \n",
    "                                     epochs=25,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = image_only_model.predict([X_Train_bulk_img_holdout,X_Train_inter_img_hodlout]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_pred_i, y_true_i in zip(y_pred, Y_Train_Hg_porosity_holdout):\n",
    "    plt.plot(y_pred_i, label=\"pred\")\n",
    "    plt.plot(y_true_i,label=\"True\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_image.history[\"loss\"],label=\"train\")\n",
    "plt.plot(history_image.history[\"val_loss\"],label=\"test\")\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(history_image.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracted Porosity ONLY Training  = Porosity Only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_porosity_ony_model():    \n",
    "\n",
    "    model3 = Sequential()\n",
    "    model3.add(InputLayer(input_shape=(3,), name=\"porosity_input\"))\n",
    "    model3.add(Dense(100, activation=\"relu\"))\n",
    "\n",
    "    dense_1 = Dense(100, activation=\"tanh\")( model3.output)\n",
    "    dense_2 = Dense(10, activation=\"relu\",name= \"Hg_porosity_diff\")(dense_1)\n",
    "    lambda_layer = Lambda(Cumulative_Sum,name= \"Hg_porosity\")(dense_2)\n",
    "    \n",
    "    big_model = Model(model3.input,[lambda_layer])    \n",
    "    big_model.compile(\"adam\",loss=\"mean_squared_error\")\n",
    "    return big_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porosity_only_model = build_porosity_ony_model()\n",
    "# porosity_only_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_porosity = porosity_only_model.fit(x=X_Train_porosity_train, y = Y_Train_Hg_porosity_train, \n",
    "                                           batch_size=62,validation_split=0.15, epochs=100,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = porosity_only_model.predict(X_Train_porosity_holdout) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_pred_i, y_true_i in zip(y_pred, Y_Train_Hg_porosity_holdout):\n",
    "    plt.plot(y_pred_i, label=\"pred\")\n",
    "    plt.plot(y_true_i,label=\"True\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_porosity.history[\"loss\"],label=\"train\")\n",
    "plt.plot(history_porosity.history[\"val_loss\"],label=\"test\")\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning from VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = VGG19(weights='imagenet',include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19.trainable = False\n",
    "X_Train_bulk_rgb_img_train = np.stack([X_Train_bulk_img_train.reshape(-1,224,224)]*3,axis=-1)\n",
    "X_Train_bulk_rgb_img_holdout = np.stack([X_Train_bulk_img_holdout.reshape(-1,224,224)]*3,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_bulk_VGG19_train = vgg19.predict(X_Train_bulk_rgb_img_train)\n",
    "X_Train_bulk_VGG19_holdout = vgg19.predict(X_Train_bulk_rgb_img_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_Train_bulk_VGG19_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vgg19_bulk_only_model(nfilter = 16, filter_size = 2):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(7,7,512), name=\"vgg19_img_input\"))\n",
    "    model.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))    \n",
    "    model.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(Flatten())   \n",
    "\n",
    "    dense_1 = Dense(16, activation=\"tanh\")(model.output)\n",
    "    dense_2 = Dense(10, activation=\"relu\",name= \"Hg_porosity_diff\")(dense_1)\n",
    "    lambda_layer = Lambda(Cumulative_Sum,name= \"Hg_porosity\")(dense_2)    \n",
    "    big_model = Model(model.input,lambda_layer)    \n",
    "\n",
    "    big_model.compile(\"adam\",loss=\"mean_squared_error\")\n",
    "    return big_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_bulk_only_model = build_vgg19_bulk_only_model(nfilter=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg_bulk_only_model_histoy = vgg_bulk_only_model.fit(X_Train_bulk_VGG19_train, y = Y_Train_Hg_porosity_train, \n",
    "                        batch_size=25,validation_split=0.15, epochs=50,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vgg19_pred_holdout = vgg_bulk_only_model.predict([X_Train_bulk_VGG19_holdout])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for y_pred_i, y_true_i in zip(y_vgg19_pred_holdout, Y_Train_Hg_porosity_holdout):\n",
    "    plt.plot(y_pred_i, label=\"pred\")\n",
    "    plt.plot(y_true_i,label=\"True\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vgg_bulk_only_model_histoy.history[\"loss\"],label=\"train\")\n",
    "plt.plot(vgg_bulk_only_model_histoy.history[\"val_loss\"],label=\"test\")\n",
    "plt.legend()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1 = df_total[\"Sample\"]==df_files[\"Sample\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Alone Models\n",
    "#500X Images Alone based on ADAM Optimisation and VGG16 ARchitecture\n",
    "def build_image_only_model_500X(nfilter = 16, filter_size = 3):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(224,224), name=\"bulk_img_input\"))\n",
    "    model.add(Reshape((224,224,1)))\n",
    "    model.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Convolution2D(nfilter,filter_size,strides=(filter_size,filter_size), padding=\"valid\"))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    dense_1 = Dense(100, activation=\"tanh\")(model.output)\n",
    "    dense_2 = Dense(10, activation=\"relu\", name= \"Hg_porosity_diff\")(dense_1)\n",
    "    lambda_layer = Lambda(Cumulative_Sum,name= \"Hg_porosity\")(dense_2)    \n",
    "    big_model = Model([model.input],[lambda_layer])\n",
    "    #big_model = Model([model.input, model2.input],[dense_2])\n",
    "\n",
    "    big_model.compile(\"adam\",loss=\"mean_squared_error\")\n",
    "    return big_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_only_model_500X = build_image_only_model_500X()\n",
    "image_only_model_500X.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_total.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_files.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Roughness Parameter Models\n",
    "#Roughness Scalar Parameter Alone\n",
    "\n",
    "def build_roughness_scalar_model():    \n",
    "    model3 = Sequential()\n",
    "    model3.add(InputLayer(input_shape=(3,), name=\"roughness_input\"))\n",
    "    model3.add(Dense(100, activation=\"relu\"))\n",
    "\n",
    "    dense_1 = Dense(100, activation=\"tanh\")( model3.output)\n",
    "    dense_2 = Dense(10, activation=\"relu\",name= \"Hg_porosity_diff\")(dense_1)\n",
    "    lambda_layer = Lambda(Cumulative_Sum,name= \"Hg_porosity\")(dense_2)\n",
    "    \n",
    "    big_model = Model(model3.input,[lambda_layer])    \n",
    "    big_model.compile(\"adam\",loss=\"mean_squared_error\")\n",
    "    return big_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roughness_scalar_model = build_roughness_scalar_model()\n",
    "roughness_scalar_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roughness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hg_df = Hg_porosity_df.drop_duplicates(subset=['Sample'])\n",
    "Hg_df.reset_index(drop=True, inplace=True)\n",
    "Hg_df.drop(columns=['Image_ID', 'Slice_ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Hg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_roughness_scalar_train = df_roughness[[\"Ra_(microns)\",\"Rq_(microns)\",\"Rz_(microns)\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_roughness_scalar = roughness_scalar_model.fit(x=X_Train_roughness_scalar_train, \n",
    "                                           y = Y_Train_Hg_porosity_train, batch_size=62,validation_split=0.15, epochs=100,verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
